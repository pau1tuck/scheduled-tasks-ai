
==== signals.soon.py ====
from django.dispatch import receiver
from django_q.signals import post_execute


@receiver(post_execute)
def create_task_record(sender, task, **kwargs):
    TaskRecord.objects.update_or_create(
        task=task,
        defaults={
            "task_name": task.name or task.func,
            "status": "Success" if task.success else "Failed",
            "started_at": task.started,
            "completed_at": task.stopped,
            "result": str(task.result) if task.result else None,
            "error": str(task.result) if not task.success else None,
        },
    )

==== insights-app.txt ====

==== data_pipeline.py ====
# apps/insights/data_pipeline.py
"""
Title: Data Pipeline for CSV Processing and LLM Integration
Description:
This script orchestrates the data pipeline for processing GA4 CSV data.
It validates, cleans, filters, and generates statistical overviews,
and optionally integrates with an LLM for dataset summaries.

Usage:
The `run_pipeline` function can be imported and called programmatically:
    from apps.insights.data_pipeline import run_pipeline
"""

import logging
import os
import pandas as pd
from apps.insights.services.csv_processor import CSVProcessor
from apps.insights.services.openai.summary_generator import generate_summary

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def run_pipeline(file_path: str, start_date: str):
    """
    Orchestrates the CSV processing pipeline and outputs results.
    Args:
        file_path (str): Path to the CSV file.
        start_date (str): Starting date for data filtering (YYYY-MM-DD).
    """
    try:
        logging.info("Initializing CSVProcessor...")
        processor = CSVProcessor(file_path)

        logging.info("Starting the CSV processing pipeline...")
        processor.load()
        processor.validate()
        processor.clean()
        week1_df, week2_df = processor.filter(start_date)

        start_date_dt = pd.to_datetime(start_date)
        week1_start = start_date_dt
        week1_end = week1_start + pd.Timedelta(days=6)
        week2_start = week1_end + pd.Timedelta(days=1)
        week2_end = week2_start + pd.Timedelta(days=6)

        logging.info("Generating statistical overviews...")
        logging.info(
            f"\nStatistical Overview - Week 1 (Start: {week1_start.date()}, End: {week1_end.date()}):"
        )
        print(week1_df.describe().to_string())

        logging.info(
            f"\nStatistical Overview - Week 2 (Start: {week2_start.date()}, End: {week2_end.date()}):"
        )
        print(week2_df.describe().to_string())

        logging.info("Generating summaries with OpenAI...")
        week1_summary = week1_df.describe().to_string()
        week2_summary = week2_df.describe().to_string()

        week1_llm_summary = generate_summary(week1_summary)
        week2_llm_summary = generate_summary(week2_summary)

        logging.info(
            f"\nLLM Summary - Week 1 ({week1_start.date()} to {week1_end.date()}):"
        )
        print(week1_llm_summary.dataset_summary)
        logging.info("Key Metrics:")
        for metric in week1_llm_summary.key_metrics:
            print(f"{metric.name}: {metric.value} ({metric.description})")

        logging.info(
            f"\nLLM Summary - Week 2 ({week2_start.date()} to {week2_end.date()}):"
        )
        print(week2_llm_summary.dataset_summary)
        logging.info("Key Metrics:")
        for metric in week2_llm_summary.key_metrics:
            print(f"{metric.name}: {metric.value} ({metric.description})")

        logging.info("Pipeline executed successfully!")

    except Exception as e:
        logging.error(f"Pipeline failed: {e}")
        raise

==== __init__.py ====

==== apps.py ====
from django.apps import AppConfig


class InsightsConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "apps.insights"

==== dump_project.py ====
import os

output_file = "insights-app.txt"
exclude_dir = "./env"
file_types = (".py", ".js", ".css", ".html", ".yaml", ".json", ".conf", ".txt")

with open(output_file, "w") as out:
    for root, dirs, files in os.walk("."):
        # Exclude the env directory and its subdirectories
        dirs[:] = [d for d in dirs if os.path.join(root, d) != exclude_dir]

        for file in files:
            if file.endswith(file_types):
                file_path = os.path.join(root, file)
                out.write(f"\n==== {file} ====\n")
                with open(file_path, "r", encoding="utf-8") as f:
                    out.write(f.read())

==== forms.py ====
# apps/insights/forms.py
from django import forms


class RunComparisonForm(forms.Form):
    start_date = forms.DateField(
        widget=forms.widgets.DateInput(attrs={"type": "date"}),
        label="Start Date",
        help_text="Select the start date for running the comparison.",
    )

==== admin.py ====
# apps/insights/admin.py
from django.contrib import admin
from django.urls import path
from django.shortcuts import render, redirect
from .forms import RunComparisonForm
from .models.comparison import Comparison, KeyMetricComparison
from .models.summary import Summary, KeyMetric


class KeyMetricInline(admin.TabularInline):
    """
    Inline admin to display all KeyMetric entries for a Summary.
    """

    model = KeyMetric
    extra = 0  # Do not display extra blank rows
    readonly_fields = ("name", "formatted_value")
    can_delete = False

    def formatted_value(self, obj):
        """Display the value rounded to the nearest whole number."""
        return f"{round(obj.value):,}" if obj.value is not None else "N/A"

    formatted_value.short_description = "Value (Rounded)"


class KeyMetricComparisonInline(admin.TabularInline):
    """
    Inline admin to display all KeyMetricComparison entries for a Comparison.
    """

    model = KeyMetricComparison
    extra = 0  # Do not display extra blank rows
    readonly_fields = (
        "name",
        "rounded_value1",
        "rounded_value2",
        "description",
        "formatted_percentage_difference",
    )
    can_delete = False

    def rounded_value1(self, obj):
        """Round value1 to the nearest whole number."""
        return f"{round(obj.value1):,}" if obj.value1 is not None else "N/A"

    def rounded_value2(self, obj):
        """Round value2 to the nearest whole number."""
        return f"{round(obj.value2):,}" if obj.value2 is not None else "N/A"

    def formatted_percentage_difference(self, obj):
        """Display percentage difference to 1 decimal place."""
        return (
            f"{obj.percentage_difference:.1f}%"
            if obj.percentage_difference is not None
            else "N/A"
        )

    rounded_value1.short_description = "Week 1 Value (Rounded)"
    rounded_value2.short_description = "Week 2 Value (Rounded)"
    formatted_percentage_difference.short_description = "Percentage Difference"


class ComparisonAdmin(admin.ModelAdmin):
    list_display = (
        "start_date",
        "end_date",
        "comparison_summary",
        "display_summary1",
        "display_summary2",
    )
    search_fields = ("start_date", "end_date")
    inlines = [KeyMetricComparisonInline]  # Add the inline view for KeyMetricComparison

    def display_summary1(self, obj):
        """Display Summary1 details."""
        return f"Summary from {obj.summary1.start_date} to {obj.summary1.end_date}"

    def display_summary2(self, obj):
        """Display Summary2 details."""
        return f"Summary from {obj.summary2.start_date} to {obj.summary2.end_date}"

    display_summary1.short_description = "Summary 1"
    display_summary2.short_description = "Summary 2"


class SummaryAdmin(admin.ModelAdmin):
    """
    Admin view for the Summary model.
    """

    list_display = ("start_date", "end_date", "dataset_summary")
    search_fields = ("start_date", "end_date")
    readonly_fields = (
        "start_date",
        "end_date",
        "dataset_summary",
    )  # Make fields read-only
    inlines = [KeyMetricInline]  # Add inline view for KeyMetric


admin.site.register(Summary, SummaryAdmin)  # Register the Summary model
admin.site.register(Comparison, ComparisonAdmin)  # Register the Comparison model

==== tests.py ====
from django.test import TestCase

# Create your tests here.

==== views.py ====
from django.shortcuts import render

# Create your views here.

==== __init__.py ====

==== 0002_comparison_keymetric_keymetriccomparison_summary_and_more.py ====
# Generated by Django 5.1.3 on 2024-11-24 04:33

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('insights', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='Comparison',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('comparison_summary', models.TextField(help_text='A concise summary of differences and similarities between the two summaries.')),
                ('start_date', models.DateField(editable=False, help_text='Start date of the comparison, derived from summary1.')),
                ('end_date', models.DateField(editable=False, help_text='End date of the comparison, derived from summary2.')),
            ],
            options={
                'ordering': ['-created_at'],
            },
        ),
        migrations.CreateModel(
            name='KeyMetric',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('name', models.CharField(help_text='Name of the metric.', max_length=100)),
                ('value', models.FloatField(help_text='Numeric value of the metric.')),
            ],
            options={
                'ordering': ['name'],
            },
        ),
        migrations.CreateModel(
            name='KeyMetricComparison',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('name', models.CharField(help_text='Name of the metric being compared.', max_length=100)),
                ('value1', models.FloatField(help_text='Value from the first summary.')),
                ('value2', models.FloatField(help_text='Value from the second summary.')),
                ('description', models.TextField(blank=True, help_text='Description of the observed difference or trend.', null=True)),
                ('percentage_difference', models.FloatField(blank=True, help_text='Percentage difference between the two values.', null=True)),
                ('comparison', models.ForeignKey(help_text='The comparison this key metric comparison belongs to.', on_delete=django.db.models.deletion.CASCADE, related_name='key_metrics_comparison', to='insights.comparison')),
            ],
            options={
                'ordering': ['name'],
                'unique_together': {('comparison', 'name')},
            },
        ),
        migrations.CreateModel(
            name='Summary',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('start_date', models.DateField(help_text='Start date of the data period.')),
                ('end_date', models.DateField(help_text='End date of the data period.')),
                ('dataset_summary', models.TextField(help_text='A concise English summary of the dataset.')),
                ('data_source', models.CharField(blank=True, help_text='File path or identifier of the data source.', max_length=255, null=True)),
            ],
            options={
                'verbose_name_plural': 'Summaries',
                'ordering': ['-start_date'],
                'unique_together': {('start_date', 'end_date')},
            },
        ),
        migrations.DeleteModel(
            name='DataSummary',
        ),
        migrations.AddField(
            model_name='keymetric',
            name='summary',
            field=models.ForeignKey(help_text='The summary this key metric belongs to.', on_delete=django.db.models.deletion.CASCADE, related_name='key_metrics', to='insights.summary'),
        ),
        migrations.AddField(
            model_name='comparison',
            name='summary1',
            field=models.ForeignKey(help_text='The first summary being compared.', on_delete=django.db.models.deletion.CASCADE, related_name='comparisons_as_summary1', to='insights.summary'),
        ),
        migrations.AddField(
            model_name='comparison',
            name='summary2',
            field=models.ForeignKey(help_text='The second summary being compared.', on_delete=django.db.models.deletion.CASCADE, related_name='comparisons_as_summary2', to='insights.summary'),
        ),
        migrations.AlterUniqueTogether(
            name='keymetric',
            unique_together={('summary', 'name')},
        ),
        migrations.AlterUniqueTogether(
            name='comparison',
            unique_together={('summary1', 'summary2')},
        ),
    ]

==== 0001_initial.py ====
# Generated by Django 5.1.3 on 2024-11-23 13:12

from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='DataSummary',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('label', models.CharField(help_text="A label identifying the dataset (e.g., 'Week 1').", max_length=50)),
                ('plain_summary', models.TextField(help_text='An English summary of the dataset.')),
                ('key_metrics', models.JSONField(help_text='Structured key metrics from the dataset.')),
                ('metadata', models.JSONField(blank=True, help_text='Additional metadata about the summary (e.g., filters applied).', null=True)),
            ],
            options={
                'verbose_name': 'Data Summary',
                'verbose_name_plural': 'Data Summaries',
            },
        ),
    ]

==== test_summary_service.py ====
# apps/insights/tests/test_summary_service.py

import os
import pytest
from apps.insights.services.summary_service import process_week
from apps.insights.services.openai.schemas import SummaryOutput
from django.conf import settings

print(f"SECRET_KEY in Test: {settings.SECRET_KEY}")


def test_process_week():
    """
    Test the summary service with the actual CSV file and a fixed start date.
    Prints the output for manual verification.
    """
    file_path = os.path.join(os.path.dirname(__file__), "../data/ga4_data.csv")
    start_date = "2024-01-08"
    week_number = 1  # Testing for Week 1

    try:
        # Process the week
        result = process_week(file_path, start_date, week_number)

        # Verify the output type
        assert isinstance(
            result, SummaryOutput
        ), "Result is not a SummaryOutput object."

        # Print dataset summary and key metrics for manual verification
        print("Dataset Summary:")
        print(result.dataset_summary)
        print("\nKey Metrics:")
        for metric in result.key_metrics:
            print(f"{metric.name}: {metric.value}")

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Summary service test failed: {e}")

==== summary_output_1.json ====
{
    "start_date": "2024-01-01",
    "end_date": "2024-01-07",
    "dataset_summary": "The dataset represents a one-week overview of web traffic and e-commerce performance metrics, captured between January 1, 2024, and January 7, 2024. The average daily sessions were 1543, with a notably high variance, peaking at 1940 sessions, suggesting variable user interest or marketing influence. The number of users averaged 1265 daily, with new users making up approximately 34% of the total user base. The data indicates a healthy engagement with an average of 4 pages viewed per session, and the sessions lasted an average of 163 seconds. The bounce rate, at approximately 20%, indicates that one in five users left after viewing only one page, which may warrant investigation into the landing page effectiveness. Conversion rates were low, at 2.78%, but the transactions averaged over 34 per day, leading to a significant average daily revenue of $1622. This suggests that while few users converted, those who did generate meaningful revenue. Spikes or drops in any of these metrics could be tied to external promotional efforts or changes in the website content itself, warranting further exploration.",
    "key_metrics": [
        {
            "name": "Average Sessions",
            "value": 1543.428571
        },
        {
            "name": "Average Users",
            "value": 1265.142857
        },
        {
            "name": "Average New Users",
            "value": 427.285714
        },
        {
            "name": "Average Pageviews",
            "value": 6225.857143
        },
        {
            "name": "Pages per Session",
            "value": 4.008571
        },
        {
            "name": "Average Session Duration",
            "value": 163.1
        },
        {
            "name": "Bounce Rate",
            "value": 0.199686
        },
        {
            "name": "Conversion Rate",
            "value": 0.0278
        },
        {
            "name": "Average Transactions",
            "value": 34.142857
        },
        {
            "name": "Average Revenue",
            "value": 1622.525714
        }
    ]
}
==== test_comparison_generator.py ====
# apps/insights/tests/test_comparison_generator.py

import pytest
from apps.insights.services.openai.comparison_generator import generate_comparison
from apps.insights.services.openai.schemas import ComparisonOutput, KeyMetricComparison


def test_generate_comparison():
    """
    Test the generate_comparison function using two pre-formatted dataset summary strings.
    Prints the output for manual verification.
    """
    # Pre-formatted dataset summaries
    summary1 = """
    The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
    Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
    such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
    Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
    over the specified time frame.

    Key Metrics:
    - Average Sessions: 1543.43
    - Average Users: 1265.14
    - Average New Users: 427.29
    - Average Pageviews: 6225.86
    - Pages per Session: 4.01
    - Average Session Duration: 163.1
    - Bounce Rate: 0.2
    - Conversion Rate: 0.028
    - Average Transactions: 34.14
    - Average Revenue: 1622.53
    """

    summary2 = """
    The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
    from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
    average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
    approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
    of 6891.71 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
    16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
    in a daily revenue averaging $2087.17.

    Key Metrics:
    - Average Sessions: 1682.57
    - Average Users: 1237.86
    - Average New Users: 424.14
    - Average Pageviews: 6891.71
    - Pages per Session: 4.07
    - Average Session Duration: 153.88
    - Bounce Rate: 0.1606
    - Conversion Rate: 0.0425
    - Average Transactions: 49.43
    - Average Revenue: 2087.17
    """

    try:
        # Call the generator
        result = generate_comparison(summary1, summary2)

        # Verify the output type
        assert isinstance(
            result, ComparisonOutput
        ), "Result is not a ComparisonOutput object."

        # Log and print results for manual verification
        print("Comparison Summary:")
        print(result.comparison_summary)
        print("\nKey Metrics Comparison:")
        for metric in result.key_metrics_comparison:
            print(
                f"{metric.name}: "
                f"Week 1 Value = {metric.value1}, "
                f"Week 2 Value = {metric.value2} "
                f"({metric.description})"
            )

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Comparison generator test failed: {e}")

==== spec_summary_service.py ====
# Run this in the Django Python shell inside the Docker container
from apps.insights.services.summary_service import process_week
from apps.insights.models.summary import Summary, KeyMetric

# Replace with your test CSV file path and start_date
file_path = (
    "apps/insights/data/ga4_data.csv"  # Ensure the path is accessible in the container
)
start_date = "2024-01-01"  # Replace with your test date
week_number = 1  # Testing for Week 1

# Call the service
try:
    result = process_week(file_path, start_date, week_number)
    print("LLM Summary Output:", result.dataset_summary)

    # Check database entries
    print("Summaries in DB:", Summary.objects.all())
    print("Key Metrics in DB:", KeyMetric.objects.all())

except Exception as e:
    print(f"Error during test: {e}")

==== test_task_scheduler.py ====
import os
import pytest
import time
from django_q.tasks import async_task, result
import redis


# Ensure migrations are applied for the test database
@pytest.fixture(autouse=True)
def apply_migrations(db):
    from django.core.management import call_command

    call_command("migrate")


@pytest.mark.django_db
def test_process_week_task():
    """
    Test the process_week_task function via async_task and print the result.
    """
    # Ensure critical environment variables are set
    os.environ.setdefault("DJANGO_SECRET_KEY", "test_secret_key")
    os.environ.setdefault("REDIS_URL", "redis://redis:6379/5")
    os.environ.setdefault("REDIS_HOST", "redis")
    os.environ.setdefault("REDIS_PORT", "6379")
    os.environ.setdefault("REDIS_DB", "5")

    # Debug environment variables
    print("DEBUG: Environment Variables:")
    print(f"REDIS_URL: {os.getenv('REDIS_URL')}")
    print(f"REDIS_HOST: {os.getenv('REDIS_HOST')}")
    print(f"REDIS_PORT: {os.getenv('REDIS_PORT')}")
    print(f"REDIS_DB: {os.getenv('REDIS_DB')}")

    # Ensure the Redis client connects properly
    try:
        redis_client = redis.Redis.from_url(os.getenv("REDIS_URL"))
        assert redis_client.ping(), "Redis connection failed!"
        print("DEBUG: Redis connection successful.")
    except Exception as redis_error:
        pytest.fail(f"Redis setup failed: {redis_error}")

    # Correctly resolve the file path for the input data
    file_path = os.path.abspath(
        os.path.join(os.path.dirname(__file__), "../data/ga4_data.csv")
    )
    print(f"DEBUG: Resolved file path for GA4 data: {file_path}")
    start_date = "2024-01-01"  # Example start date
    week_number = 1  # Testing for Week 1

    # Trigger the async task and wait for the result
    try:
        # Trigger the task via Django-Q
        task_id = async_task(
            "apps.insights.tasks.process_week_task",
            file_path,
            start_date,
            week_number,
        )
        print(f"DEBUG: Task {task_id} triggered successfully.")

        # Wait for the result with retries
        retries = 10
        result_data = None
        while retries > 0:
            result_data = result(task_id)
            if result_data is not None:
                break
            retries -= 1
            time.sleep(1)  # Wait 1 second before retrying

        # Validate and print the result
        assert result_data is not None, "Task did not return a result."
        print("DEBUG: Task Result:")
        print(result_data)

    except Exception as task_error:
        pytest.fail(f"Task test failed: {task_error}")

==== test_comparison_service.py ====
# apps/insights/tests/test_comparison_service.py

import pytest
from apps.insights.services.comparison_service import process_comparison
from apps.insights.services.openai.schemas import ComparisonOutput, KeyMetricComparison


def test_process_comparison():
    """
    Test the comparison service using mock structured data for Week 1 and Week 2.
    Prints the output for manual verification.
    """
    # Week 1 structured data
    data_summary1 = {
        "dataset_summary": """
        The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
        Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
        such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
        Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
        over the specified time frame.
        """,
        "key_metrics": [
            {
                "name": "Average Sessions",
                "value": 1543.43,
            },
            {
                "name": "Average Users",
                "value": 1265.14,
            },
            {
                "name": "Average New Users",
                "value": 427.29,
            },
            {
                "name": "Average Pageviews",
                "value": 6225.86,
            },
            {
                "name": "Pages per Session",
                "value": 4.01,
            },
            {
                "name": "Average Session Duration",
                "value": 163.1,
            },
            {
                "name": "Bounce Rate",
                "value": 0.2,
            },
            {
                "name": "Conversion Rate",
                "value": 0.028,
            },
            {
                "name": "Average Transactions",
                "value": 34.14,
            },
            {
                "name": "Average Revenue",
                "value": 1622.53,
            },
        ],
    }

    # Week 2 structured data
    data_summary2 = {
        "dataset_summary": """
        The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
        from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
        average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
        approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
        of 6892 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
        16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
        in a daily revenue averaging $2087.17.
        """,
        "key_metrics": [
            {
                "name": "Average Sessions",
                "value": 1682.57,
            },
            {
                "name": "Average Users",
                "value": 1237.86,
            },
            {
                "name": "Average New Users",
                "value": 424.14,
            },
            {
                "name": "Average Pageviews",
                "value": 6891.71,
            },
            {
                "name": "Pages per Session",
                "value": 4.07,
            },
            {
                "name": "Average Session Duration",
                "value": 153.88,
            },
            {
                "name": "Bounce Rate",
                "value": 0.1606,
            },
            {
                "name": "Conversion Rate",
                "value": 0.0425,
            },
            {
                "name": "Average Transactions",
                "value": 49.43,
            },
            {
                "name": "Average Revenue",
                "value": 2087.17,
            },
        ],
    }

    try:
        # Process comparison
        result = process_comparison(data_summary1, data_summary2)

        # Verify the output type
        assert isinstance(
            result, ComparisonOutput
        ), "Result is not a ComparisonOutput object."

        # Print comparison summary and key metrics for manual verification
        print("Comparison Summary:")
        print(result.comparison_summary)
        print("\nKey Metrics Comparison:")
        for metric in result.key_metrics_comparison:
            print(
                f"{metric.name}: Week 1 Value = {metric.value1}, Week 2 Value = {metric.value2} ({metric.description})"
            )

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Comparison service test failed: {e}")

==== spec_comparison_service.py ====
# Run this in the Django Python shell inside the Docker container
from apps.insights.services.comparison_service import (
    process_comparison,
    save_comparison_to_database,
    save_comparison_to_file,
)
from apps.insights.models.comparison import Comparison, KeyMetricComparison
from apps.insights.models.summary import Summary
from apps.insights.services.openai.schemas import ComparisonOutput

# Mock structured data for Week 1 and Week 2
data_summary1 = {
    "dataset_summary": """
    The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
    Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
    such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
    Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
    over the specified time frame.
    """,
    "key_metrics": [
        {"name": "Average Sessions", "value": 1543.43},
        {"name": "Average Users", "value": 1265.14},
        {"name": "Average New Users", "value": 427.29},
        {"name": "Average Pageviews", "value": 6225.86},
        {"name": "Pages per Session", "value": 4.01},
        {"name": "Average Session Duration", "value": 163.1},
        {"name": "Bounce Rate", "value": 0.2},
        {"name": "Conversion Rate", "value": 0.028},
        {"name": "Average Transactions", "value": 34.14},
        {"name": "Average Revenue", "value": 1622.53},
    ],
}

data_summary2 = {
    "dataset_summary": """
    The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
    from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
    average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
    approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
    of 6892 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
    16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
    in a daily revenue averaging $2087.17.
    """,
    "key_metrics": [
        {"name": "Average Sessions", "value": 1682.57},
        {"name": "Average Users", "value": 1237.86},
        {"name": "Average New Users", "value": 424.14},
        {"name": "Average Pageviews", "value": 6891.71},
        {"name": "Pages per Session", "value": 4.07},
        {"name": "Average Session Duration", "value": 153.88},
        {"name": "Bounce Rate", "value": 0.1606},
        {"name": "Conversion Rate", "value": 0.0425},
        {"name": "Average Transactions", "value": 49.43},
        {"name": "Average Revenue", "value": 2087.17},
    ],
}

# Run the comparison service
try:
    # Ensure you have at least two summary objects in your database for testing
    summary1 = Summary.objects.first()
    summary2 = Summary.objects.last()

    if not summary1 or not summary2:
        raise ValueError(
            "Not enough summaries in the database. Please ensure two exist."
        )

    # Process the comparison
    comparison_result = process_comparison(data_summary1, data_summary2)

    # Save the comparison to the database
    save_comparison_to_database(summary1.id, summary2.id, comparison_result)

    # Optionally save the comparison to a JSON file
    save_comparison_to_file(comparison_result, summary1.id, summary2.id)

    # Output results for manual verification
    print("Comparison Summary:")
    print(comparison_result.comparison_summary)
    print("\nKey Metrics Comparison:")
    for metric in comparison_result.key_metrics_comparison:
        print(
            f"{metric.name}: Week 1 Value = {metric.value1}, Week 2 Value = {metric.value2} ({metric.description})"
        )

    # Verify database entries
    print("Comparison in DB:", Comparison.objects.all())
    print("Key Metric Comparisons in DB:", KeyMetricComparison.objects.all())

except Exception as e:
    print(f"Error during test: {e}")

==== __init__.py ====
from .summary import Summary, KeyMetric
from .comparison import Comparison, KeyMetricComparison

__all__ = ["Summary", "KeyMetric", "Comparison", "KeyMetricComparison"]

==== comparison.py ====
# apps/insights/models/comparison.py
from django.db import models
from apps.common.behaviors.timestampable import Timestampable  # Importing Timestampable
from apps.insights.models.summary import (
    Summary,
)  # Ensure this import aligns with project structure


class Comparison(Timestampable):
    """
    Model to store the comparison between two summaries.
    """

    summary1 = models.ForeignKey(
        Summary,
        related_name="comparisons_as_summary1",
        on_delete=models.CASCADE,
        help_text="The first summary being compared.",
    )
    summary2 = models.ForeignKey(
        Summary,
        related_name="comparisons_as_summary2",
        on_delete=models.CASCADE,
        help_text="The second summary being compared.",
    )
    comparison_summary = models.TextField(
        help_text="A concise summary of differences and similarities between the two summaries."
    )
    start_date = models.DateField(
        help_text="Start date of the comparison, derived from summary1.",
        editable=False,
    )
    end_date = models.DateField(
        help_text="End date of the comparison, derived from summary2.",
        editable=False,
    )

    def save(self, *args, **kwargs):
        self.start_date = self.summary1.start_date
        self.end_date = self.summary2.end_date
        super().save(*args, **kwargs)

    def __str__(self):
        return f"Comparison from {self.start_date} to {self.end_date}"

    class Meta:
        unique_together = ("summary1", "summary2")
        ordering = ["-created_at"]


class KeyMetricComparison(Timestampable):
    """
    Model to store individual key metric comparisons related to a Comparison.
    """

    comparison = models.ForeignKey(
        Comparison,
        related_name="key_metrics_comparison",
        on_delete=models.CASCADE,
        help_text="The comparison this key metric comparison belongs to.",
    )
    name = models.CharField(
        max_length=100, help_text="Name of the metric being compared."
    )
    value1 = models.FloatField(help_text="Value from the first summary.")
    value2 = models.FloatField(help_text="Value from the second summary.")
    description = models.TextField(
        help_text="Description of the observed difference or trend.",
        null=True,
        blank=True,
    )
    percentage_difference = models.FloatField(
        help_text="Percentage difference between the two values.", null=True, blank=True
    )

    def save(self, *args, **kwargs):
        if self.value1 and self.value2:
            self.percentage_difference = (
                ((self.value2 - self.value1) / self.value1) * 100
                if self.value1 != 0
                else None
            )
        super().save(*args, **kwargs)

    def __str__(self):
        return f"{self.name} Comparison (Comparison ID: {self.comparison.id})"

    class Meta:
        unique_together = ("comparison", "name")
        ordering = ["name"]

==== summary.py ====
# apps/insights/models/summary.py
from django.db import models
from apps.common.behaviors.timestampable import Timestampable


class Summary(Timestampable):
    """
    Model to store the dataset summary and key metrics for a specific time period.
    """

    start_date = models.DateField(help_text="Start date of the data period.")
    end_date = models.DateField(help_text="End date of the data period.")
    dataset_summary = models.TextField(
        help_text="A concise English summary of the dataset."
    )
    data_source = models.CharField(
        max_length=255,
        null=True,
        blank=True,
        help_text="File path or identifier of the data source.",
    )

    def __str__(self):
        return f"Summary from {self.start_date} to {self.end_date}"

    class Meta:
        ordering = ["-start_date"]
        unique_together = ("start_date", "end_date")  # Removed `summary_type` here
        verbose_name_plural = "Summaries"


class KeyMetric(Timestampable):
    """
    Model to store individual key metrics related to a Summary.
    """

    summary = models.ForeignKey(
        Summary,
        related_name="key_metrics",
        on_delete=models.CASCADE,
        help_text="The summary this key metric belongs to.",
    )
    name = models.CharField(max_length=100, help_text="Name of the metric.")
    value = models.FloatField(help_text="Numeric value of the metric.")

    def __str__(self):
        return f"{self.name}: {self.value} (Summary ID: {self.summary.id})"

    class Meta:
        unique_together = ("summary", "name")
        ordering = ["name"]

==== task_record.py ====
from django.db import models
from django_q.models import Task


class TaskRecord(models.Model):
    """
    Model to record additional information about tasks executed via Django-Q2.
    """

    task = models.OneToOneField(
        Task,
        on_delete=models.CASCADE,
        help_text="The Django-Q task associated with this record.",
    )
    task_name = models.CharField(
        max_length=255, help_text="Name of the task function being executed."
    )
    status = models.CharField(
        max_length=50, help_text="Status of the task (e.g., Pending, Success, Failed)."
    )
    started_at = models.DateTimeField(help_text="Timestamp when the task started.")
    completed_at = models.DateTimeField(
        null=True, blank=True, help_text="Timestamp when the task completed."
    )
    result = models.TextField(
        null=True, blank=True, help_text="Result of the task, if applicable."
    )
    error = models.TextField(
        null=True, blank=True, help_text="Error message if the task failed."
    )
    summary = models.ForeignKey(
        Summary,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="Associated summary, if applicable.",
    )
    comparison = models.ForeignKey(
        Comparison,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="Associated comparison, if applicable.",
    )
    start_date = models.DateField(
        null=True,
        blank=True,
        help_text="Start date of the data period the task is processing.",
    )
    end_date = models.DateField(
        null=True,
        blank=True,
        help_text="End date of the data period the task is processing.",
    )

    def __str__(self):
        return f"Task Record for Task ID: {self.task.id}"

    class Meta:
        verbose_name = "Task Record"
        verbose_name_plural = "Task Records"
        ordering = ["-started_at"]

==== comparison_changelist.html ====
{% extends "admin/change_list.html" %}

{% block content %}
  <a href="{{ start_comparison_url }}" class="button">Run Comparison</a>
  {{ block.super }}
  <p>DEBUG: Custom changelist template loaded.</p>
{% endblock %}
==== run_pipeline.html ====
{% extends "admin/base_site.html" %}

{% block content %}
  <h1>{{ title }}</h1>
  <form method="post">
      {% csrf_token %}
      {{ form.as_p }}
      <button type="submit" class="button">Start Comparison</button>
  </form>
  <p>DEBUG: Custom run pipeline template loaded.</p>
{% endblock %}
==== start_comparison.html ====
{% extends "admin/base_site.html" %}

{% block content %}
  <form method="post">
      {% csrf_token %}
      {{ form.as_p }}
      <button type="submit" class="button">Run Comparison</button>
  </form>
{% endblock %}
==== csv_processor.py ====
# apps/insights/services/csv_processor.py
import logging
from .csv.csv_reader import load_csv
from .csv.data_validator import validate_columns
from .csv.data_cleaner import clean_data
from .csv.data_filter import filter_data
from .csv.data_overview import generate_overview

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class CSVProcessor:
    def __init__(self, file_path: str):
        """
        Initialize the CSVProcessor with the path to the CSV file.
        """
        self.file_path = file_path
        self.df = None  # Placeholder for the loaded DataFrame

    def load(self):
        """
        Load the CSV file into a Pandas DataFrame.
        """
        logging.info("Loading CSV...")
        self.df = load_csv(self.file_path)

    def validate(self):
        """
        Validate that the CSV contains all required columns.
        """
        logging.info("Validating CSV columns...")
        validate_columns(self.df)

    def clean(self):
        """
        Clean the DataFrame by standardizing and formatting columns.
        """
        logging.info("Cleaning data...")
        self.df = clean_data(self.df)

    def filter(self, start_date: str):
        """
        Filter the data into two weeks based on the start date.
        """
        logging.info("Filtering data into Week 1 and Week 2...")
        return filter_data(self.df, start_date)

    def generate_overview(self, df, label):
        """
        Generate a statistical overview for a single DataFrame.
        """
        logging.info(f"Generating statistical overview for {label}...")
        print(f"\nStatistical Overview - {label}:")
        print(df.describe())

    # def process(self, start_date: str, week_number: int):
    #     try:
    #         self.load()
    #         self.validate()
    #         self.clean()
    #         week_df = self.filter(start_date)[week_number - 1]
    #         self.generate_overview(week_df, f"Week {week_number}")
    #         return week_df  # Return the processed week DataFrame
    #     except ValueError as e:
    #         logging.error(f"Processing error: {e}")
    #         raise

==== comparison_service.py ====
# apps/insights/services/comparison_service.py
"""
Comparison Service for Dataset Summaries
Handles LLM comparison generation and logging for two dataset summaries.
"""

import json
import logging
from django.db import transaction
from apps.insights.models.comparison import Comparison, KeyMetricComparison
from apps.insights.models.summary import Summary
from apps.insights.services.openai.comparison_generator import generate_comparison
from apps.insights.services.openai.schemas import ComparisonOutput

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def prepare_summary(data_summary: dict) -> str:
    """
    Combines dataset_summary and key_metrics from a structured dataset summary into a single string for LLM input.

    Args:
        data_summary (dict): A dictionary containing 'dataset_summary' (str) and 'key_metrics' (list of dicts).

    Returns:
        str: A combined string representation of the dataset summary and its key metrics.
    """
    try:
        if not data_summary.get("dataset_summary"):
            raise ValueError("Missing 'dataset_summary' in data_summary.")

        if not data_summary.get("key_metrics"):
            raise ValueError("Missing 'key_metrics' in data_summary.")

        key_metrics_str = "\n".join(
            f"- {metric['name']}: {metric['value']}"
            for metric in data_summary["key_metrics"]
            if "name" in metric and "value" in metric
        )

        if not key_metrics_str:
            logging.warning("Key metrics are empty or malformed.")

        return f"{data_summary['dataset_summary']}\n\nKey Metrics:\n{key_metrics_str}"
    except Exception as e:
        logging.error(f"Failed to prepare summary: {e}")
        raise


def process_comparison(data_summary1: dict, data_summary2: dict) -> ComparisonOutput:
    """
    Processes two dataset summaries, merges them into strings, and generates a structured comparison.

    Args:
        data_summary1 (dict): The first dataset summary (with 'dataset_summary' and 'key_metrics').
        data_summary2 (dict): The second dataset summary.

    Returns:
        ComparisonOutput: A structured comparison containing a summary and key metrics comparison.
    """
    try:
        logging.info("Starting comparison of dataset summaries...")

        # Validate and prepare text strings for the LLM
        summary1 = prepare_summary(data_summary1)
        summary2 = prepare_summary(data_summary2)

        logging.info("Generated summaries for comparison.")
        logging.debug(f"Summary 1: {summary1}")
        logging.debug(f"Summary 2: {summary2}")

        # Generate comparison using LLM
        comparison_result = generate_comparison(summary1, summary2)

        # Log detailed results
        logging.info("Comparison completed successfully.")
        logging.debug(f"Raw comparison result: {comparison_result}")

        logging.info("Comparison Summary:")
        logging.info(comparison_result.comparison_summary)
        logging.info("Key Metrics Comparison:")
        for metric in comparison_result.key_metrics_comparison:
            logging.info(
                f"{metric.name}: Week 1 Value = {metric.value1}, "
                f"Week 2 Value = {metric.value2} ({metric.description})"
            )

        return comparison_result

    except ValueError as ve:
        logging.error(f"Validation Error: {ve}")
        raise

    except Exception as e:
        logging.error(f"Unexpected error during comparison: {e}")
        raise


def save_comparison_to_database(
    summary1_id: int, summary2_id: int, comparison_result: ComparisonOutput
):
    """
    Save the LLM comparison result into the database.

    Args:
        summary1_id (int): ID of the first summary (Week 1).
        summary2_id (int): ID of the second summary (Week 2).
        comparison_result (ComparisonOutput): The structured comparison result from LLM.
    """
    try:
        with transaction.atomic():
            # Fetch the summaries
            summary1 = Summary.objects.get(id=summary1_id)
            summary2 = Summary.objects.get(id=summary2_id)

            logging.info(
                f"Saving comparison for summaries {summary1_id} and {summary2_id}..."
            )

            # Create the Comparison object
            comparison = Comparison.objects.create(
                summary1=summary1,
                summary2=summary2,
                comparison_summary=comparison_result.comparison_summary,
            )

            # Create KeyMetricComparison objects
            for metric in comparison_result.key_metrics_comparison:
                KeyMetricComparison.objects.create(
                    comparison=comparison,
                    name=metric.name,
                    value1=metric.value1,
                    value2=metric.value2,
                    description=metric.description,
                )

            logging.info(
                f"Comparison saved successfully for summaries {summary1_id} and {summary2_id}."
            )

    except Summary.DoesNotExist as e:
        logging.error(f"Summary not found: {e}")
        raise ValueError(f"Summary not found: {e}")
    except Exception as e:
        logging.error(f"Failed to save comparison to the database: {e}")
        raise


def save_comparison_to_file(
    comparison_result: ComparisonOutput, summary1_id: int, summary2_id: int
):
    """
    Saves the structured comparison result to a JSON file resembling the database entry.

    Args:
        comparison_result (ComparisonOutput): The structured comparison result.
        summary1_id (int): The database ID of the first summary.
        summary2_id (int): The database ID of the second summary.
    """
    try:
        file_path = "comparison_output.json"
        logging.info(f"Saving comparison result to {file_path}...")

        # Construct the data dictionary to match database structure
        data = {
            "summary1": summary1_id,
            "summary2": summary2_id,
            "comparison_summary": comparison_result.comparison_summary,
            "key_metrics_comparison": [
                {
                    "name": metric.name,
                    "value1": metric.value1,
                    "value2": metric.value2,
                    "description": metric.description,
                    "percentage_difference": (
                        ((metric.value2 - metric.value1) / metric.value1) * 100
                        if metric.value1 != 0
                        else None
                    ),
                }
                for metric in comparison_result.key_metrics_comparison
            ],
        }

        # Write to the JSON file
        with open(file_path, "w") as json_file:
            json.dump(data, json_file, indent=4)

        logging.info("Comparison result saved successfully.")
    except Exception as e:
        logging.error(f"Failed to save comparison result to file: {e}")
        raise

==== __init__.py ====

==== summary_service.py ====
# apps/insights/services/summary_service.py
"""
Summary Service for Single-Week Data Processing
Handles CSV data validation, processing, LLM summary generation, and key metric extraction for a single week.

This service processes a single week's data from a CSV file, generating a summary and key metrics using OpenAI's LLM, and saving the results to both the database and a JSON file. It uses the CSVProcessor to load, validate, clean, and filter data based on the provided start date. A statistical overview is generated for the specified week, which is then summarized into a dataset summary and key metrics. The results are stored in the Summary and KeyMetric models and saved as JSON for debugging or visualization. Errors are logged at each step.

"""

import json
import logging
from django.db import transaction
from apps.insights.models.summary import Summary, KeyMetric
from apps.insights.services.csv_processor import CSVProcessor
from apps.insights.services.openai.summary_generator import generate_summary
from apps.insights.services.openai.schemas import SummaryOutput
import pandas as pd
import datetime  # Added import for datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def process_week(file_path: str, start_date: str, week_number: int) -> SummaryOutput:
    """
    Processes a single week's data and generates an LLM summary.

    Args:
        file_path (str): Path to the CSV file.
        start_date (str): Start date for the week (YYYY-MM-DD).
        week_number (int): Week number (1 or 2).

    Returns:
        SummaryOutput: LLM summary and key metrics for the week.
    """
    try:
        logging.info(
            f"Processing Week {week_number} data starting from {start_date}..."
        )

        # Initialize CSVProcessor and load data
        processor = CSVProcessor(file_path)
        processor.load()
        processor.validate()
        processor.clean()

        # Verify the 'date' column type
        if not pd.api.types.is_datetime64_any_dtype(processor.df["date"]):
            logging.error("The 'date' column is not in datetime format after cleaning.")
            raise TypeError("Date column is not datetime")

        # Filter data for the specified week
        week_df = processor.filter(start_date)[week_number - 1]

        # Verify the 'date' column type in week_df
        if not pd.api.types.is_datetime64_any_dtype(week_df["date"]):
            logging.error("The 'date' column in week_df is not in datetime format.")
            raise TypeError("Date column in week_df is not datetime")

        # Generate statistical overview using CSVProcessor
        processor.generate_overview(week_df, f"Week {week_number}")

        # Generate LLM summary
        logging.info("Requesting summary from OpenAI...")
        statistical_summary = week_df.describe().to_string()
        llm_summary = generate_summary(statistical_summary)

        # Validate and log results
        logging.info(f"LLM Summary - Week {week_number}: {llm_summary.dataset_summary}")
        logging.info("Key Metrics:")
        for metric in llm_summary.key_metrics:
            logging.info(f"{metric.name}: {metric.value}")

        # Convert `date` column to datetime and extract the end date
        end_date_max = week_df["date"].max()
        logging.info(f"Max date value: {end_date_max} (type: {type(end_date_max)})")
        if isinstance(end_date_max, (pd.Timestamp, datetime.datetime)):
            end_date = end_date_max.strftime("%Y-%m-%d")
        else:
            # Attempt to convert to datetime if not already
            end_date = pd.to_datetime(end_date_max).strftime("%Y-%m-%d")

        # Save summary and metrics to the database
        save_summary_to_database(
            start_date=start_date,
            end_date=end_date,
            llm_summary=llm_summary,
        )

        # Save summary to JSON file
        save_summary_to_file(start_date, end_date, llm_summary, week_number)

        return llm_summary

    except Exception as e:
        logging.error(f"Failed to process Week {week_number}: {e}")
        raise


def save_summary_to_database(
    start_date: str, end_date: str, llm_summary: SummaryOutput
):
    """
    Saves the structured summary result and its key metrics to the database.

    Args:
        start_date (str): Start date for the summary (YYYY-MM-DD).
        end_date (str): End date for the summary (YYYY-MM-DD).
        llm_summary (SummaryOutput): The structured summary result.
    """
    try:
        with transaction.atomic():  # Ensure all-or-nothing database operations
            logging.info(
                f"Saving summary for {start_date} to {end_date} to the database..."
            )

            # Create and save the Summary instance
            summary = Summary.objects.create(
                start_date=start_date,
                end_date=end_date,
                dataset_summary=llm_summary.dataset_summary,
            )

            # Create and save the KeyMetric instances
            for metric in llm_summary.key_metrics:
                KeyMetric.objects.create(
                    summary=summary,
                    name=metric.name,
                    value=metric.value,
                )

            logging.info(
                f"Summary and key metrics for {start_date} to {end_date} saved successfully."
            )

    except Exception as e:
        logging.error(f"Failed to save summary and key metrics to the database: {e}")
        raise


def save_summary_to_file(
    start_date: str, end_date: str, llm_summary: SummaryOutput, week_number: int
):
    """
    Saves the structured summary result to a JSON file in the same format as the database.

    Args:
        start_date (str): Start date for the summary (YYYY-MM-DD).
        end_date (str): End date for the summary (YYYY-MM-DD).
        llm_summary (SummaryOutput): The structured summary result.
        week_number (int): The week number being processed.
    """
    try:
        file_path = f"summary_output_week_{week_number}.json"
        logging.info(f"Saving Week {week_number} summary result to {file_path}...")

        # Construct the data dictionary to match database structure
        data = {
            "start_date": start_date,
            "end_date": end_date,
            "dataset_summary": llm_summary.dataset_summary,
            "key_metrics": [
                {"name": metric.name, "value": metric.value}
                for metric in llm_summary.key_metrics
            ],
        }

        # Write to the JSON file
        with open(file_path, "w") as json_file:
            json.dump(data, json_file, indent=4)

        logging.info(f"Week {week_number} summary result saved successfully.")
    except Exception as e:
        logging.error(f"Failed to save Week {week_number} summary result to file: {e}")
        raise

==== data_overview.py ====
# apps/insights/services/csv/data_overview.py


def generate_overview(self, df, label):
    """
    Generate a statistical overview for the given DataFrame.

    Args:
        df (DataFrame): The DataFrame to generate an overview for.
        label (str): A label for logging (e.g., Week 1 or Week 2).
    """
    logging.info(f"Generating statistical overview for {label}...")
    print(f"\nStatistical Overview - {label}:")
    print(df.describe())

==== data_validator.py ====
# apps/insights/services/csv/data_validator.py
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

REQUIRED_COLUMNS = {
    "date",
    "source",
    "sessions",
    "users",
    "new_users",
    "pageviews",
    "pages_per_session",
    "avg_session_duration",
    "bounce_rate",
    "conversion_rate",
    "transactions",
    "revenue",
}


def validate_columns(df):
    # Check for missing required columns
    missing_columns = REQUIRED_COLUMNS - set(df.columns)
    if missing_columns:
        raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")
    logging.info("All required key columns are present.")

==== data_filter.py ====
# apps/insights/services/csv/data_filter.py
import pandas as pd
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def filter_data(df, start_date):
    """
    Filter the DataFrame into two weeks based on the start_date.

    Args:
        df (pd.DataFrame): Input DataFrame to filter.
        start_date (str): Start date for filtering (YYYY-MM-DD).

    Returns:
        tuple: Two DataFrames (Week 1, Week 2).
    """
    logging.info("Filtering data for organic traffic...")
    organic_df = df[df["source"] == "organic"]
    if organic_df.empty:
        raise ValueError("No data found for organic traffic.")

    # Define date ranges
    logging.info(f"Calculating date ranges from start_date: {start_date}")
    start_date = pd.to_datetime(start_date)
    end_date_week1 = start_date + pd.Timedelta(days=6)  # Week 1 range
    start_date_week2 = start_date + pd.Timedelta(days=7)  # Week 2 range
    end_date_week2 = start_date + pd.Timedelta(days=13)  # Week 2 range end

    # Filter Week 1
    logging.info(f"Filtering Week 1: {start_date.date()} to {end_date_week1.date()}")
    week1_df = organic_df[
        (organic_df["date"] >= start_date) & (organic_df["date"] <= end_date_week1)
    ]
    if week1_df.empty:
        raise ValueError("No data found for Week 1.")

    # Log filtered Week 1 data
    logging.info(f"Week 1 Data (Rows: {len(week1_df)}):\n{week1_df}")

    # Filter Week 2
    logging.info(
        f"Filtering Week 2: {start_date_week2.date()} to {end_date_week2.date()}"
    )
    week2_df = organic_df[
        (organic_df["date"] >= start_date_week2)
        & (organic_df["date"] <= end_date_week2)
    ]
    if week2_df.empty:
        raise ValueError("No data found for Week 2.")

    # Log filtered Week 2 data
    logging.info(f"Week 2 Data (Rows: {len(week2_df)}):\n{week2_df}")

    return week1_df, week2_df

==== csv_reader.py ====
# apps/insights/services/csv/csv_reader.py
import pandas as pd  # Missing import added


def load_csv(file_path: str) -> pd.DataFrame:
    """
    Load a CSV file into a Pandas DataFrame.

    Args:
        file_path (str): Path to the CSV file.

    Returns:
        pd.DataFrame: Loaded data.
    """
    try:
        df = pd.read_csv(file_path)
        print(
            f"Successfully loaded {file_path}: {len(df)} rows, {len(df.columns)} columns"
        )
        return df
    except Exception as e:
        raise ValueError(f"Error loading CSV file: {e}")

==== __init__.py ====

==== data_cleaner.py ====
# apps/insights/services/csv/data_cleaner.py
import pandas as pd
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


# Detect the date column dynamically
def detect_date_column(df):
    date_columns = [col for col in df.columns if "date" in col.lower()]
    if len(date_columns) == 0:
        raise ValueError("No date column detected in the dataset.")
    if len(date_columns) > 1:
        raise ValueError(f"Multiple possible date columns found: {date_columns}")
    logging.info(f"Date column detected: {date_columns[0]}")
    return date_columns[0]


# Standardize the format of the date column
def standardize_date_format(df, date_column):
    try:
        df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
        if df[date_column].isna().any():
            raise ValueError(f"Invalid or unparseable dates in column '{date_column}'.")
        # Removed the following line to keep 'date' as datetime
        # df[date_column] = df[date_column].dt.strftime("%Y-%m-%d")
        logging.info(
            f"Dates standardized to datetime format in column '{date_column}'."
        )
        return df
    except Exception as e:
        raise ValueError(f"Error standardizing date column: {e}")


# Ensure date column is in datetime format for filtering
def ensure_datetime_format(df, date_column):
    try:
        df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
        if df[date_column].isna().any():
            raise ValueError(f"Invalid or unparseable dates in column '{date_column}'.")
        logging.info(f"Date column '{date_column}' confirmed as datetime format.")
        return df
    except Exception as e:
        raise ValueError(f"Error ensuring datetime format: {e}")


# Perform the full data cleaning process
def clean_data(df):
    date_column = detect_date_column(df)
    df = standardize_date_format(df, date_column)
    df = ensure_datetime_format(df, date_column)
    return df

==== summary_generator.py ====
# apps/insights/services/openai/summary_generator.py

import os
from instructor import from_openai
from openai import OpenAI
from .schemas import SummaryOutput
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load OpenAI API key
openai_api_key = os.environ.get("OPENAI_API_KEY")  # Use os.environ.get()

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment variables.")

# Initialize OpenAI client
client = from_openai(OpenAI(api_key=openai_api_key))


def generate_summary(statistical_summary: str) -> SummaryOutput:
    """
    Generates a structured dataset summary using OpenAI API.

    Args:
        statistical_summary (str): Statistical summary of the dataset.

    Returns:
        SummaryOutput: A structured summary containing dataset insights and key metrics.
    """
    prompt = f"""
You are a data analyst tasked with summarizing a dataset. The following is a statistical summary of the dataset:

{statistical_summary}

Please provide the summary in the following JSON format:

{{
    "dataset_summary": "A concise, insightful summary highlighting significant findings, trends, or patterns observed in the data. Mention any notable data or anomalies in the key metrics, providing context by referencing the actual values and what they indicate about user behavior or performance metrics.",
    "key_metrics": [
        {{
            "name": "Name of Metric",
            "value": Numeric value
        }}
        // Repeat for each key metric
    ]
}}

Ensure that:
- All numeric values are provided as numbers (not strings).
- The key_metrics include the following metrics in this order:
    - "Average Sessions"
    - "Average Users"
    - "Average New Users"
    - "Average Pageviews"
    - "Pages per Session"
    - "Average Session Duration"
    - "Bounce Rate"
    - "Conversion Rate"
    - "Average Transactions"
    - "Average Revenue"
- Do not include descriptions for the key metrics.
- Focus on delivering specific insights derived from the data.
- Avoid generic statements or repeating information without analysis.
"""

    try:
        logging.info("Requesting dataset summary from OpenAI...")

        # API call with structured output validation
        response = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            messages=[{"role": "user", "content": prompt}],
            response_model=SummaryOutput,
        )

        # Log the raw response from OpenAI for debugging
        logging.info(f"Raw LLM response: {response.json()}")

        logging.info("Successfully received structured response.")
        return response

    except client.ValidationError as e:
        logging.error(f"Validation error: {e}")
        raise ValueError(f"Validation error: {e}")

    except client.ApiError as e:
        logging.error(f"API error: {e}")
        raise ValueError(f"API error: {e}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        raise ValueError(f"Unexpected error: {e}")

==== __init__.py ====

==== comparison_generator.py ====
# apps/insights/services/openai/comparison_generator.py

import os
from instructor import from_openai
from openai import OpenAI
from .schemas import ComparisonOutput
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load OpenAI API key
openai_api_key = os.environ.get("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment variables.")

# Initialize OpenAI client
client = from_openai(OpenAI(api_key=openai_api_key))


def generate_comparison(summary1: str, summary2: str) -> ComparisonOutput:
    """
    Generates a structured comparison between two dataset summaries using the OpenAI API.

    Args:
        summary1 (str): The first dataset summary as a string (Week 1).
        summary2 (str): The second dataset summary as a string (Week 2).

    Returns:
        ComparisonOutput: A structured comparison containing a summary and key metrics comparison.
    """
    prompt = f"""
You are a data analyst tasked with comparing two dataset summaries. Here are the summaries:

Week 1:
{summary1}

Week 2:
{summary2}

Please provide the comparison in the following JSON format:

{{
    "comparison_summary": "A concise summary of differences and similarities between Week 1 and Week 2, including notable trends or observations.",
    "key_metrics_comparison": [
        {{
            "name": "Name of Metric",
            "value1": Value from Week 1,
            "value2": Value from Week 2,
            "description": "Description of the observed difference or trend, including specific figures and percentages where appropriate."
        }}
        // Repeat for each key metric
    ]
}}

Ensure that:
- All numeric values are provided as numbers (not strings).
- The key_metrics_comparison includes the following metrics in this order:
    - "Average Sessions"
    - "Average Users"
    - "Average New Users"
    - "Average Pageviews"
    - "Pages per Session"
    - "Average Session Duration"
    - "Bounce Rate"
    - "Conversion Rate"
    - "Average Transactions"
    - "Average Revenue"
- The description for each metric explains the difference or trend observed between Week 1 and Week 2, using precise figures (e.g., differences, percentages).
- Refer to the summaries as "Week 1" and "Week 2" in your descriptions.
"""

    try:
        logging.info("Requesting dataset comparison from OpenAI...")

        # API call with structured output validation
        response = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            messages=[{"role": "user", "content": prompt}],
            response_model=ComparisonOutput,
        )

        # Log the raw response from OpenAI for debugging
        logging.info(f"Raw LLM response: {response.json()}")

        logging.info("Successfully received structured response.")
        return response

    except client.ValidationError as e:
        logging.error(f"Validation error: {e}")
        raise ValueError(f"Validation error: {e}")

    except client.ApiError as e:
        logging.error(f"API error: {e}")
        raise ValueError(f"API error: {e}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        raise ValueError(f"Unexpected error: {e}")

==== schemas.py ====
# apps/insights/services/openai/schemas.py

from pydantic import BaseModel, Field
from typing import List


class KeyMetric(BaseModel):
    """
    Represents a single key metric extracted from the dataset summary.
    """

    name: str
    value: float

    @classmethod
    def ordered_metrics(cls) -> List["KeyMetric"]:
        """
        Defines the exact order and expected names for key metrics.
        """
        return [
            cls(name="Average Sessions", value=0),
            cls(name="Average Users", value=0),
            cls(name="Average New Users", value=0),
            cls(name="Average Pageviews", value=0),
            cls(name="Pages per Session", value=0),
            cls(name="Average Session Duration", value=0),
            cls(name="Bounce Rate", value=0),
            cls(name="Conversion Rate", value=0),
            cls(name="Average Transactions", value=0),
            cls(name="Average Revenue", value=0),
        ]

    def validate_name(self) -> bool:
        """
        Ensures that the name of the metric matches one of the expected names.
        """
        expected_names = [metric.name for metric in self.ordered_metrics()]
        if self.name not in expected_names:
            raise ValueError(f"Unexpected metric name: {self.name}")
        return True


class SummaryOutput(BaseModel):
    """
    Structured output for a dataset summary response from the LLM.
    """

    dataset_summary: str = Field(
        ..., description="A concise English summary of the dataset."
    )
    key_metrics: List[KeyMetric] = Field(
        ..., description="List of key metrics extracted from the dataset."
    )

    def enforce_ordered_metrics(self):
        """
        Enforces that key metrics are in the exact order defined by `KeyMetric.ordered_metrics`.
        """
        ordered_names = [metric.name for metric in KeyMetric.ordered_metrics()]
        self.key_metrics = sorted(
            self.key_metrics,
            key=lambda metric: (
                ordered_names.index(metric.name)
                if metric.name in ordered_names
                else float("inf")
            ),
        )
        # Ensure no unexpected metrics
        for metric in self.key_metrics:
            metric.validate_name()


class KeyMetricComparison(BaseModel):
    """
    Represents a comparison of a key metric between two datasets.
    """

    name: str
    value1: float
    value2: float
    description: str  # Retaining description for comparisons


class ComparisonOutput(BaseModel):
    """
    Structured output for comparing two dataset summaries.
    """

    comparison_summary: str = Field(
        ...,
        description="A concise English summary highlighting differences and similarities between Week 1 and Week 2.",
    )
    key_metrics_comparison: List[KeyMetricComparison] = Field(
        ...,
        description="Key metrics with values from both weeks and descriptions of differences.",
    )
