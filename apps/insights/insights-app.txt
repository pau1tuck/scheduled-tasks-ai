
==== signals.soon.py ====
from django.dispatch import receiver
from django_q.signals import post_execute


@receiver(post_execute)
def create_task_record(sender, task, **kwargs):
    TaskRecord.objects.update_or_create(
        task=task,
        defaults={
            "task_name": task.name or task.func,
            "status": "Success" if task.success else "Failed",
            "started_at": task.started,
            "completed_at": task.stopped,
            "result": str(task.result) if task.result else None,
            "error": str(task.result) if not task.success else None,
        },
    )

==== insights-app.txt ====

==== tasks.py ====
# apps/insights/tasks.py

from django_q.tasks import schedule
from django.utils import timezone
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


def schedule_summary_tasks(start_date):
    """
    Schedule tasks to process summaries for Week 1 and Week 2.
    """
    # Convert start_date to timezone-aware datetime
    if isinstance(start_date, str):
        start_date = datetime.strptime(start_date, "%Y-%m-%d")
    if timezone.is_naive(start_date):
        start_date = timezone.make_aware(start_date, timezone.get_current_timezone())

    group_id = f"summary-{start_date.isoformat()}"
    next_run_time = timezone.now() + timedelta(seconds=5)

    # Schedule Task 1: Week 1
    task_1_id = schedule(
        "apps.insights.services.summary_service.process_week",
        start_date.strftime("%Y-%m-%d"),
        1,
        name=f"Week 1 Summary Task - {start_date.date()}",
        schedule_type="O",
        next_run=next_run_time,
        q_options={"group": group_id},
    )
    logger.info(f"Scheduled Task 1 for Week 1 with ID {task_1_id}")

    # Schedule Task 2: Week 2
    task_2_id = schedule(
        "apps.insights.services.summary_service.process_week",
        start_date.strftime("%Y-%m-%d"),
        2,
        name=f"Week 2 Summary Task - {start_date.date()}",
        schedule_type="O",
        next_run=next_run_time + timedelta(seconds=5),  # Starts 5 seconds after Task 1
        q_options={"group": group_id},
    )
    logger.info(f"Scheduled Task 2 for Week 2 with ID {task_2_id}")

==== __init__.py ====

==== apps.py ====
from django.apps import AppConfig


class InsightsConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "apps.insights"

==== dump_project.py ====
import os

output_file = "insights-app.txt"
exclude_dir = "./env"
file_types = (".py", ".js", ".css", ".html", ".yaml", ".json", ".conf", ".txt")

with open(output_file, "w") as out:
    for root, dirs, files in os.walk("."):
        # Exclude the env directory and its subdirectories
        dirs[:] = [d for d in dirs if os.path.join(root, d) != exclude_dir]

        for file in files:
            if file.endswith(file_types):
                file_path = os.path.join(root, file)
                out.write(f"\n==== {file} ====\n")
                with open(file_path, "r", encoding="utf-8") as f:
                    out.write(f.read())

==== forms.py ====
# apps/insights/forms.py
from django import forms


class RunComparisonForm(forms.Form):
    start_date = forms.DateField(
        widget=forms.widgets.DateInput(attrs={"type": "date"}),
        label="Start Date",
        help_text="Select the start date for running the comparison.",
    )

==== admin.py ====
# apps/insights/admin.py
from django.contrib import admin
from django.urls import path
from django.shortcuts import render, redirect
from .forms import RunComparisonForm
from .models.comparison import Comparison, KeyMetricComparison
from .models.summary import Summary, KeyMetric


class KeyMetricInline(admin.TabularInline):
    """
    Inline admin to display all KeyMetric entries for a Summary.
    """

    model = KeyMetric
    extra = 0  # Do not display extra blank rows
    readonly_fields = ("name", "formatted_value")
    can_delete = False

    def formatted_value(self, obj):
        """Display the value rounded to the nearest whole number."""
        return f"{round(obj.value):,}" if obj.value is not None else "N/A"

    formatted_value.short_description = "Value (Rounded)"


class KeyMetricComparisonInline(admin.TabularInline):
    """
    Inline admin to display all KeyMetricComparison entries for a Comparison.
    """

    model = KeyMetricComparison
    extra = 0  # Do not display extra blank rows
    readonly_fields = (
        "name",
        "rounded_value1",
        "rounded_value2",
        "description",
        "formatted_percentage_difference",
    )
    fields = readonly_fields  # Make all fields explicitly read-only
    can_delete = False

    def rounded_value1(self, obj):
        """Round value1 to the nearest whole number."""
        return f"{round(obj.value1):,}" if obj.value1 is not None else "N/A"

    def rounded_value2(self, obj):
        """Round value2 to the nearest whole number."""
        return f"{round(obj.value2):,}" if obj.value2 is not None else "N/A"

    def formatted_percentage_difference(self, obj):
        """Display percentage difference to 1 decimal place."""
        return (
            f"{obj.percentage_difference:.1f}%"
            if obj.percentage_difference is not None
            else "N/A"
        )

    rounded_value1.short_description = "Week 1 Value (Rounded)"
    rounded_value2.short_description = "Week 2 Value (Rounded)"
    formatted_percentage_difference.short_description = "Percentage Difference"


class ComparisonAdmin(admin.ModelAdmin):
    list_display = (
        "comparison_start_date",
        "comparison_summary",
        "display_summary1",
        "display_summary2",
    )
    search_fields = ("summary1__start_date", "summary2__start_date")
    inlines = [KeyMetricComparisonInline]  # Add the inline view for KeyMetricComparison

    def comparison_start_date(self, obj):
        """Use the earliest start_date from summary1 for consistency."""
        return obj.summary1.start_date

    comparison_start_date.short_description = "Start Date"

    def display_summary1(self, obj):
        """Display Summary1 details."""
        return f"Summary from {obj.summary1.start_date}"

    def display_summary2(self, obj):
        """Display Summary2 details."""
        return f"Summary from {obj.summary2.start_date}"

    display_summary1.short_description = "Summary 1"
    display_summary2.short_description = "Summary 2"


class SummaryAdmin(admin.ModelAdmin):
    """
    Admin view for the Summary model.
    """

    list_display = ("start_date", "dataset_summary")
    search_fields = ("start_date",)
    readonly_fields = (
        "start_date",
        "dataset_summary",
    )  # Make fields read-only
    inlines = [KeyMetricInline]  # Add inline view for KeyMetric


admin.site.register(Summary, SummaryAdmin)  # Register the Summary model
admin.site.register(Comparison, ComparisonAdmin)  # Register the Comparison model

==== tests.py ====
from django.test import TestCase

# Create your tests here.

==== views.py ====
from django.shortcuts import render

# Create your views here.

==== __init__.py ====

==== 0002_remove_comparison_end_date_and_more.py ====
# Generated by Django 5.1.3 on 2024-11-24 16:29

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('insights', '0001_initial'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='comparison',
            name='end_date',
        ),
        migrations.AlterUniqueTogether(
            name='summary',
            unique_together={('start_date',)},
        ),
        migrations.RemoveField(
            model_name='summary',
            name='end_date',
        ),
    ]

==== 0001_initial.py ====
# Generated by Django 5.1.3 on 2024-11-24 15:36

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Summary',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('start_date', models.DateField(help_text='Start date of the data period.')),
                ('end_date', models.DateField(help_text='End date of the data period.')),
                ('dataset_summary', models.TextField(help_text='A concise English summary of the dataset.')),
                ('data_source', models.CharField(blank=True, help_text='File path or identifier of the data source.', max_length=255, null=True)),
            ],
            options={
                'verbose_name_plural': 'Summaries',
                'ordering': ['-start_date'],
                'unique_together': {('start_date', 'end_date')},
            },
        ),
        migrations.CreateModel(
            name='Comparison',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('comparison_summary', models.TextField(help_text='A concise summary of differences and similarities between the two summaries.')),
                ('start_date', models.DateField(editable=False, help_text='Start date of the comparison, derived from summary1.')),
                ('end_date', models.DateField(editable=False, help_text='End date of the comparison, derived from summary2.')),
                ('summary1', models.ForeignKey(help_text='The first summary being compared.', on_delete=django.db.models.deletion.CASCADE, related_name='comparisons_as_summary1', to='insights.summary')),
                ('summary2', models.ForeignKey(help_text='The second summary being compared.', on_delete=django.db.models.deletion.CASCADE, related_name='comparisons_as_summary2', to='insights.summary')),
            ],
            options={
                'ordering': ['-created_at'],
                'unique_together': {('summary1', 'summary2')},
            },
        ),
        migrations.CreateModel(
            name='KeyMetricComparison',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('name', models.CharField(help_text='Name of the metric being compared.', max_length=100)),
                ('value1', models.FloatField(help_text='Value from the first summary.')),
                ('value2', models.FloatField(help_text='Value from the second summary.')),
                ('description', models.TextField(blank=True, help_text='Description of the observed difference or trend.', null=True)),
                ('percentage_difference', models.FloatField(blank=True, help_text='Percentage difference between the two values.', null=True)),
                ('comparison', models.ForeignKey(help_text='The comparison this key metric comparison belongs to.', on_delete=django.db.models.deletion.CASCADE, related_name='key_metrics_comparison', to='insights.comparison')),
            ],
            options={
                'ordering': ['name'],
                'unique_together': {('comparison', 'name')},
            },
        ),
        migrations.CreateModel(
            name='KeyMetric',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('modified_at', models.DateTimeField(auto_now=True)),
                ('name', models.CharField(help_text='Name of the metric.', max_length=100)),
                ('value', models.FloatField(help_text='Numeric value of the metric.')),
                ('summary', models.ForeignKey(help_text='The summary this key metric belongs to.', on_delete=django.db.models.deletion.CASCADE, related_name='key_metrics', to='insights.summary')),
            ],
            options={
                'ordering': ['name'],
                'unique_together': {('summary', 'name')},
            },
        ),
    ]

==== test_summary_service.py ====
# apps/insights/tests/test_summary_service.py

import os
import pytest
from apps.insights.services.summary_service import process_week
from apps.insights.services.openai.schemas import SummaryOutput
from django.conf import settings

print(f"SECRET_KEY in Test: {settings.SECRET_KEY}")


def test_process_week():
    """
    Test the summary service with the actual CSV file and a fixed start date.
    Prints the output for manual verification.
    """
    file_path = os.path.join(os.path.dirname(__file__), "../data/ga4_data.csv")
    start_date = "2024-01-08"
    week_number = 1  # Testing for Week 1

    try:
        # Process the week
        result = process_week(file_path, start_date, week_number)

        # Verify the output type
        assert isinstance(
            result, SummaryOutput
        ), "Result is not a SummaryOutput object."

        # Print dataset summary and key metrics for manual verification
        print("Dataset Summary:")
        print(result.dataset_summary)
        print("\nKey Metrics:")
        for metric in result.key_metrics:
            print(f"{metric.name}: {metric.value}")

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Summary service test failed: {e}")

==== test_comparison_generator.py ====
# apps/insights/tests/test_comparison_generator.py

import pytest
from apps.insights.services.openai.comparison_generator import generate_comparison
from apps.insights.services.openai.schemas import ComparisonOutput, KeyMetricComparison


def test_generate_comparison():
    """
    Test the generate_comparison function using two pre-formatted dataset summary strings.
    Prints the output for manual verification.
    """
    # Pre-formatted dataset summaries
    summary1 = """
    The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
    Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
    such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
    Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
    over the specified time frame.

    Key Metrics:
    - Average Sessions: 1543.43
    - Average Users: 1265.14
    - Average New Users: 427.29
    - Average Pageviews: 6225.86
    - Pages per Session: 4.01
    - Average Session Duration: 163.1
    - Bounce Rate: 0.2
    - Conversion Rate: 0.028
    - Average Transactions: 34.14
    - Average Revenue: 1622.53
    """

    summary2 = """
    The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
    from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
    average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
    approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
    of 6891.71 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
    16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
    in a daily revenue averaging $2087.17.

    Key Metrics:
    - Average Sessions: 1682.57
    - Average Users: 1237.86
    - Average New Users: 424.14
    - Average Pageviews: 6891.71
    - Pages per Session: 4.07
    - Average Session Duration: 153.88
    - Bounce Rate: 0.1606
    - Conversion Rate: 0.0425
    - Average Transactions: 49.43
    - Average Revenue: 2087.17
    """

    try:
        # Call the generator
        result = generate_comparison(summary1, summary2)

        # Verify the output type
        assert isinstance(
            result, ComparisonOutput
        ), "Result is not a ComparisonOutput object."

        # Log and print results for manual verification
        print("Comparison Summary:")
        print(result.comparison_summary)
        print("\nKey Metrics Comparison:")
        for metric in result.key_metrics_comparison:
            print(
                f"{metric.name}: "
                f"Week 1 Value = {metric.value1}, "
                f"Week 2 Value = {metric.value2} "
                f"({metric.description})"
            )

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Comparison generator test failed: {e}")

==== test_comparison_service.py ====
# apps/insights/tests/test_comparison_service.py

import pytest
from apps.insights.services.comparison_service import process_comparison
from apps.insights.services.openai.schemas import ComparisonOutput, KeyMetricComparison


def test_process_comparison():
    """
    Test the comparison service using mock structured data for Week 1 and Week 2.
    Prints the output for manual verification.
    """
    # Week 1 structured data
    data_summary1 = {
        "dataset_summary": """
        The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
        Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
        such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
        Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
        over the specified time frame.
        """,
        "key_metrics": [
            {
                "name": "Average Sessions",
                "value": 1543.43,
            },
            {
                "name": "Average Users",
                "value": 1265.14,
            },
            {
                "name": "Average New Users",
                "value": 427.29,
            },
            {
                "name": "Average Pageviews",
                "value": 6225.86,
            },
            {
                "name": "Pages per Session",
                "value": 4.01,
            },
            {
                "name": "Average Session Duration",
                "value": 163.1,
            },
            {
                "name": "Bounce Rate",
                "value": 0.2,
            },
            {
                "name": "Conversion Rate",
                "value": 0.028,
            },
            {
                "name": "Average Transactions",
                "value": 34.14,
            },
            {
                "name": "Average Revenue",
                "value": 1622.53,
            },
        ],
    }

    # Week 2 structured data
    data_summary2 = {
        "dataset_summary": """
        The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
        from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
        average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
        approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
        of 6892 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
        16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
        in a daily revenue averaging $2087.17.
        """,
        "key_metrics": [
            {
                "name": "Average Sessions",
                "value": 1682.57,
            },
            {
                "name": "Average Users",
                "value": 1237.86,
            },
            {
                "name": "Average New Users",
                "value": 424.14,
            },
            {
                "name": "Average Pageviews",
                "value": 6891.71,
            },
            {
                "name": "Pages per Session",
                "value": 4.07,
            },
            {
                "name": "Average Session Duration",
                "value": 153.88,
            },
            {
                "name": "Bounce Rate",
                "value": 0.1606,
            },
            {
                "name": "Conversion Rate",
                "value": 0.0425,
            },
            {
                "name": "Average Transactions",
                "value": 49.43,
            },
            {
                "name": "Average Revenue",
                "value": 2087.17,
            },
        ],
    }

    try:
        # Process comparison
        result = process_comparison(data_summary1, data_summary2)

        # Verify the output type
        assert isinstance(
            result, ComparisonOutput
        ), "Result is not a ComparisonOutput object."

        # Print comparison summary and key metrics for manual verification
        print("Comparison Summary:")
        print(result.comparison_summary)
        print("\nKey Metrics Comparison:")
        for metric in result.key_metrics_comparison:
            print(
                f"{metric.name}: Week 1 Value = {metric.value1}, Week 2 Value = {metric.value2} ({metric.description})"
            )

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Comparison service test failed: {e}")

==== __init__.py ====
from .summary import Summary, KeyMetric
from .comparison import Comparison, KeyMetricComparison

__all__ = ["Summary", "KeyMetric", "Comparison", "KeyMetricComparison"]

==== comparison.py ====
# apps/insights/models/comparison.py
from django.db import models
from apps.common.behaviors.timestampable import Timestampable  # Importing Timestampable
from apps.insights.models.summary import (
    Summary,
)  # Ensure this import aligns with project structure


class Comparison(Timestampable):
    """
    Model to store the comparison between two summaries.
    """

    summary1 = models.ForeignKey(
        Summary,
        related_name="comparisons_as_summary1",
        on_delete=models.CASCADE,
        help_text="The first summary being compared.",
    )
    summary2 = models.ForeignKey(
        Summary,
        related_name="comparisons_as_summary2",
        on_delete=models.CASCADE,
        help_text="The second summary being compared.",
    )
    comparison_summary = models.TextField(
        help_text="A concise summary of differences and similarities between the two summaries."
    )
    start_date = models.DateField(
        help_text="Start date of the comparison, derived from summary1.",
        editable=False,
    )

    def save(self, *args, **kwargs):
        self.start_date = self.summary1.start_date
        super().save(*args, **kwargs)

    def __str__(self):
        return f"Comparison from {self.start_date}"

    class Meta:
        unique_together = ("summary1", "summary2")
        ordering = ["-created_at"]


class KeyMetricComparison(Timestampable):
    """
    Model to store individual key metric comparisons related to a Comparison.
    """

    comparison = models.ForeignKey(
        Comparison,
        related_name="key_metrics_comparison",
        on_delete=models.CASCADE,
        help_text="The comparison this key metric comparison belongs to.",
    )
    name = models.CharField(
        max_length=100, help_text="Name of the metric being compared."
    )
    value1 = models.FloatField(help_text="Value from the first summary.")
    value2 = models.FloatField(help_text="Value from the second summary.")
    description = models.TextField(
        help_text="Description of the observed difference or trend.",
        null=True,
        blank=True,
    )
    percentage_difference = models.FloatField(
        help_text="Percentage difference between the two values.", null=True, blank=True
    )

    def save(self, *args, **kwargs):
        if self.value1 and self.value2:
            self.percentage_difference = (
                ((self.value2 - self.value1) / self.value1) * 100
                if self.value1 != 0
                else None
            )
        super().save(*args, **kwargs)

    def __str__(self):
        return f"{self.name} Comparison (Comparison ID: {self.comparison.id})"

    class Meta:
        unique_together = ("comparison", "name")
        ordering = ["name"]

==== summary.py ====
# apps/insights/models/summary.py
from django.db import models
from apps.common.behaviors.timestampable import Timestampable


class Summary(Timestampable):
    """
    Model to store the dataset summary and key metrics for a specific time period.
    """

    start_date = models.DateField(help_text="Start date of the data period.")
    dataset_summary = models.TextField(
        help_text="A concise English summary of the dataset."
    )
    data_source = models.CharField(
        max_length=255,
        null=True,
        blank=True,
        help_text="File path or identifier of the data source.",
    )

    def __str__(self):
        return f"Summary from {self.start_date}"

    class Meta:
        ordering = ["-start_date"]
        unique_together = ("start_date",)  # Adjusted to remove end_date
        verbose_name_plural = "Summaries"


class KeyMetric(Timestampable):
    """
    Model to store individual key metrics related to a Summary.
    """

    summary = models.ForeignKey(
        Summary,
        related_name="key_metrics",
        on_delete=models.CASCADE,
        help_text="The summary this key metric belongs to.",
    )
    name = models.CharField(max_length=100, help_text="Name of the metric.")
    value = models.FloatField(help_text="Numeric value of the metric.")

    def __str__(self):
        return f"{self.name}: {self.value} (Summary ID: {self.summary.id})"

    class Meta:
        unique_together = ("summary", "name")
        ordering = ["name"]

==== task_record.py ====
# apps/insights/models/task_record.py
from django.db import models
from django_q.models import Task
from apps.insights.models.summary import Summary
from apps.insights.models.comparison import Comparison


class TaskRecord(models.Model):
    """
    Model to record additional information about tasks executed via Django-Q2.
    """

    task = models.OneToOneField(
        Task,
        on_delete=models.CASCADE,
        help_text="The Django-Q task associated with this record.",
    )
    task_name = models.CharField(
        max_length=255, help_text="Name of the task function being executed."
    )
    status = models.CharField(
        max_length=50, help_text="Status of the task (e.g., Pending, Success, Failed)."
    )
    started_at = models.DateTimeField(help_text="Timestamp when the task started.")
    completed_at = models.DateTimeField(
        null=True, blank=True, help_text="Timestamp when the task completed."
    )
    result = models.TextField(
        null=True, blank=True, help_text="Result of the task, if applicable."
    )
    error = models.TextField(
        null=True, blank=True, help_text="Error message if the task failed."
    )
    summary = models.ForeignKey(
        Summary,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="Associated summary, if applicable.",
    )
    comparison = models.ForeignKey(
        Comparison,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="Associated comparison, if applicable.",
    )
    start_date = models.DateField(
        null=True,
        blank=True,
        help_text="Start date of the data period the task is processing.",
    )

    def __str__(self):
        return f"Task Record for Task ID: {self.task.id}"

    class Meta:
        verbose_name = "Task Record"
        verbose_name_plural = "Task Records"
        ordering = ["-started_at"]

==== comparison_changelist.html ====
{% extends "admin/change_list.html" %}

{% block content %}
  <a href="{{ start_comparison_url }}" class="button">Run Comparison</a>
  {{ block.super }}
  <p>DEBUG: Custom changelist template loaded.</p>
{% endblock %}
==== run_pipeline.html ====
{% extends "admin/base_site.html" %}

{% block content %}
  <h1>{{ title }}</h1>
  <form method="post">
      {% csrf_token %}
      {{ form.as_p }}
      <button type="submit" class="button">Start Comparison</button>
  </form>
  <p>DEBUG: Custom run pipeline template loaded.</p>
{% endblock %}
==== start_comparison.html ====
{% extends "admin/base_site.html" %}

{% block content %}
  <form method="post">
      {% csrf_token %}
      {{ form.as_p }}
      <button type="submit" class="button">Run Comparison</button>
  </form>
{% endblock %}
==== comparison_task_service.py ====
# apps/insights/services/comparison_task_service.py

from apps.insights.models.summary import Summary
from apps.insights.services.comparison_service import process_comparison
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


def run_comparison_task(start_date: str) -> dict:
    """
    Fetch summaries for Week 1 and Week 2 from the database, then pass them to the comparison service.

    Args:
        start_date (str): Start date for Week 1 in the format 'YYYY-MM-DD'.

    Returns:
        dict: The result of the comparison service.
    """
    try:
        logger.info(f"Running comparison task for start_date: {start_date}")

        # Convert start_date to datetime
        start_date_week1 = datetime.strptime(start_date, "%Y-%m-%d")
        start_date_week2 = start_date_week1 + timedelta(days=7)

        # Fetch summaries
        logger.info("Fetching summaries from the database...")
        summary1 = Summary.objects.get(start_date=start_date_week1.strftime("%Y-%m-%d"))
        summary2 = Summary.objects.get(start_date=start_date_week2.strftime("%Y-%m-%d"))

        logger.info("Summaries fetched successfully.")
        logger.info(f"Week 1 Summary ID: {summary1.id}")
        logger.info(f"Week 2 Summary ID: {summary2.id}")

        # Prepare structured data for comparison_service
        data_summary1 = {
            "dataset_summary": summary1.dataset_summary,
            "key_metrics": [
                {"name": metric.name, "value": metric.value}
                for metric in summary1.keymetric_set.all()
            ],
        }

        data_summary2 = {
            "dataset_summary": summary2.dataset_summary,
            "key_metrics": [
                {"name": metric.name, "value": metric.value}
                for metric in summary2.keymetric_set.all()
            ],
        }

        # Pass summaries to the comparison_service
        logger.info("Passing summaries to comparison_service...")
        comparison_result = process_comparison(data_summary1, data_summary2)

        logger.info("Comparison completed successfully!")
        return comparison_result

    except Summary.DoesNotExist as e:
        logger.error(f"Summary not found: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error while running comparison task: {e}")
        raise

==== csv_processor.py ====
# apps/insights/services/csv_processor.py
import logging
import pandas as pd  # Import pandas for date processing
from .csv.csv_reader import load_csv
from .csv.data_validator import validate_columns
from .csv.data_cleaner import clean_data
from .csv.data_filter import filter_data
from .csv.data_overview import generate_overview

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class CSVProcessor:
    def __init__(self):
        """
        Initialize the CSVProcessor.
        """
        self.df = None  # Placeholder for the loaded DataFrame

    def load(self):
        """
        Load the CSV file into a Pandas DataFrame.
        """
        logging.info("Loading CSV...")
        self.df = load_csv()

    def validate(self):
        """
        Validate that the CSV contains all required columns.
        """
        logging.info("Validating CSV columns...")
        validate_columns(self.df)

    def clean(self):
        """
        Clean the DataFrame by standardizing and formatting columns.
        """
        logging.info("Cleaning data...")
        self.df = clean_data(self.df)

    # FIXME: Encapsulate this logic in data_filter.py:
    def filter(self, start_date: str, week_number: int):
        """
        Filters the data for a specified week.

        Args:
            start_date (str): Start date for the dataset (YYYY-MM-DD).
            week_number (int): Week number to filter (1 = days 1-7, 2 = days 8-14).

        Returns:
            pd.DataFrame: Filtered DataFrame for the specified week.
        """
        logging.info(f"Filtering data for Week {week_number}...")
        start_date = pd.to_datetime(start_date)

        if week_number == 1:
            week_start = start_date
            week_end = start_date + pd.Timedelta(days=6)
        elif week_number == 2:
            week_start = start_date + pd.Timedelta(days=7)
            week_end = start_date + pd.Timedelta(days=13)
        else:
            raise ValueError("Invalid week number. Must be 1 or 2.")

        filtered_df = self.df[
            (self.df["date"] >= week_start) & (self.df["date"] <= week_end)
        ]
        logging.info(f"Filtered Week {week_number} Data: {len(filtered_df)} rows.")
        return filtered_df

    def generate_overview(self, df, label):
        """
        Generate a statistical overview for a single DataFrame.
        """
        logging.info(f"Generating statistical overview for {label}...")
        print(f"\nStatistical Overview - {label}:")
        print(df.describe())

==== comparison_service.py ====
# apps/insights/services/comparison_service.py
"""
Comparison Service for Dataset Summaries
Handles LLM comparison generation and logging for two dataset summaries.
"""

import json
import logging
from django.db import transaction
from apps.insights.models.comparison import Comparison, KeyMetricComparison
from apps.insights.models.summary import Summary
from apps.insights.services.openai.comparison_generator import generate_comparison
from apps.insights.services.openai.schemas import ComparisonOutput


# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def prepare_summary(data_summary: dict) -> str:
    """
    Combines dataset_summary and key_metrics from a structured dataset summary into a single string for LLM input.

    Args:
        data_summary (dict): A dictionary containing 'dataset_summary' (str) and 'key_metrics' (list of dicts).

    Returns:
        str: A combined string representation of the dataset summary and its key metrics.
    """
    try:
        if not data_summary.get("dataset_summary"):
            raise ValueError("Missing 'dataset_summary' in data_summary.")

        if not data_summary.get("key_metrics"):
            raise ValueError("Missing 'key_metrics' in data_summary.")

        key_metrics_str = "\n".join(
            f"- {metric['name']}: {metric['value']}"
            for metric in data_summary["key_metrics"]
            if "name" in metric and "value" in metric
        )

        if not key_metrics_str:
            logging.warning("Key metrics are empty or malformed.")

        return f"{data_summary['dataset_summary']}\n\nKey Metrics:\n{key_metrics_str}"
    except Exception as e:
        logging.error(f"Failed to prepare summary: {e}")
        raise


def process_comparison(data_summary1: dict, data_summary2: dict) -> ComparisonOutput:
    """
    Processes two dataset summaries, merges them into strings, and generates a structured comparison.

    Args:
        data_summary1 (dict): The first dataset summary (with 'dataset_summary' and 'key_metrics').
        data_summary2 (dict): The second dataset summary.

    Returns:
        ComparisonOutput: A structured comparison containing a summary and key metrics comparison.
    """
    try:
        logging.info("Starting comparison of dataset summaries...")

        # Validate and prepare text strings for the LLM
        summary1 = prepare_summary(data_summary1)
        summary2 = prepare_summary(data_summary2)

        logging.info("Generated summaries for comparison.")
        logging.debug(f"Summary 1: {summary1}")
        logging.debug(f"Summary 2: {summary2}")

        # Generate comparison using LLM
        comparison_result = generate_comparison(summary1, summary2)

        # Log detailed results
        logging.info("Comparison completed successfully.")
        logging.debug(f"Raw comparison result: {comparison_result}")

        logging.info("Comparison Summary:")
        logging.info(comparison_result.comparison_summary)
        logging.info("Key Metrics Comparison:")
        for metric in comparison_result.key_metrics_comparison:
            logging.info(
                f"{metric.name}: Week 1 Value = {metric.value1}, "
                f"Week 2 Value = {metric.value2} ({metric.description})"
            )

        return comparison_result

    except ValueError as ve:
        logging.error(f"Validation Error: {ve}")
        raise

    except Exception as e:
        logging.error(f"Unexpected error during comparison: {e}")
        raise


def save_comparison_to_database(
    summary1_id: int, summary2_id: int, comparison_result: ComparisonOutput
):
    """
    Save the LLM comparison result into the database.

    Args:
        summary1_id (int): ID of the first summary (Week 1).
        summary2_id (int): ID of the second summary (Week 2).
        comparison_result (ComparisonOutput): The structured comparison result from LLM.
    """
    try:
        with transaction.atomic():
            # Fetch the summaries
            summary1 = Summary.objects.get(id=summary1_id)
            summary2 = Summary.objects.get(id=summary2_id)

            logging.info(
                f"Saving comparison for summaries {summary1_id} and {summary2_id}..."
            )

            # Create the Comparison object
            comparison = Comparison.objects.create(
                summary1=summary1,
                summary2=summary2,
                comparison_summary=comparison_result.comparison_summary,
            )

            # Create KeyMetricComparison objects
            for metric in comparison_result.key_metrics_comparison:
                KeyMetricComparison.objects.create(
                    comparison=comparison,
                    name=metric.name,
                    value1=metric.value1,
                    value2=metric.value2,
                    description=metric.description,
                )

            logging.info(
                f"Comparison saved successfully for summaries {summary1_id} and {summary2_id}."
            )

    except Summary.DoesNotExist as e:
        logging.error(f"Summary not found: {e}")
        raise ValueError(f"Summary not found: {e}")
    except Exception as e:
        logging.error(f"Failed to save comparison to the database: {e}")
        raise


# def save_comparison_to_file(
#     comparison_result: ComparisonOutput, summary1_id: int, summary2_id: int
# ):
#     """
#     Saves the structured comparison result to a JSON file resembling the database entry.

#     Args:
#         comparison_result (ComparisonOutput): The structured comparison result.
#         summary1_id (int): The database ID of the first summary.
#         summary2_id (int): The database ID of the second summary.
#     """
#     try:
#         file_path = "comparison_output.json"
#         logging.info(f"Saving comparison result to {file_path}...")

#         # Construct the data dictionary to match database structure
#         data = {
#             "summary1": summary1_id,
#             "summary2": summary2_id,
#             "comparison_summary": comparison_result.comparison_summary,
#             "key_metrics_comparison": [
#                 {
#                     "name": metric.name,
#                     "value1": metric.value1,
#                     "value2": metric.value2,
#                     "description": metric.description,
#                     "percentage_difference": (
#                         ((metric.value2 - metric.value1) / metric.value1) * 100
#                         if metric.value1 != 0
#                         else None
#                     ),
#                 }
#                 for metric in comparison_result.key_metrics_comparison
#             ],
#         }

#         # Write to the JSON file
#         with open(file_path, "w") as json_file:
#             json.dump(data, json_file, indent=4)

#         logging.info("Comparison result saved successfully.")
#     except Exception as e:
#         logging.error(f"Failed to save comparison result to file: {e}")
#         raise

==== __init__.py ====

==== summary_service.py ====
# app/insights/services/summary_service.py
"""
Summary Service for Single-Week Data Processing
Handles CSV data validation, processing, LLM summary generation, and key metric extraction for a single week.

This service processes a single week's data from a CSV file, generating a summary and key metrics using OpenAI's LLM, and saving the results to both the database and a JSON file. It uses the CSVProcessor to load, validate, clean, and filter data based on the provided start date. A statistical overview is generated for the specified week, which is then summarized into a dataset summary and key metrics. The results are stored in the Summary and KeyMetric models and saved as JSON for debugging or visualization. Errors are logged at each step.

"""
import json
import logging
from django.db import transaction
from apps.insights.models.summary import Summary, KeyMetric
from apps.insights.services.csv_processor import CSVProcessor
from apps.insights.services.openai.summary_generator import generate_summary
from apps.insights.services.openai.schemas import SummaryOutput
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def process_week(start_date: str, week_number: int) -> dict:
    """
    Processes a single week's data and generates an LLM summary.

    Args:
        start_date (str): Start date for the dataset (YYYY-MM-DD).
        week_number (int): Week number to process (1 = days 1-7, 2 = days 8-14).

    Returns:
        dict: JSON-serializable dictionary containing dataset_summary and key metrics.
    """
    try:
        logging.info(
            f"Starting process_week: start_date={start_date}, week_number={week_number}"
        )

        # Adjust start_date for week_number 2
        start_date_dt = pd.to_datetime(start_date)
        if week_number == 2:
            logging.info("Adjusting start_date for week 2.")
            start_date_dt += pd.Timedelta(days=7)

        # Step 1: Initialize CSVProcessor and load data
        logging.info("Initializing CSVProcessor...")
        processor = CSVProcessor()

        logging.info("Loading CSV...")
        processor.load()
        logging.info("CSV loaded successfully.")

        # Step 2: Validate and clean
        logging.info("Validating CSV...")
        processor.validate()
        logging.info("Validation complete.")

        logging.info("Cleaning CSV...")
        processor.clean()
        logging.info("Cleaning complete.")

        # Step 3: Filter data
        logging.info(f"Filtering data for week: {week_number}")
        week_df = processor.filter(start_date_dt.strftime("%Y-%m-%d"), week_number)
        logging.info(f"Filtering complete! Filtered rows: {len(week_df)}")

        # Step 4: Generate statistical overview and LLM summary
        logging.info("Generating statistical overview...")
        processor.generate_overview(week_df, f"Week {week_number}")

        logging.info("Calling LLM to generate summary...")
        statistical_summary = week_df.describe().to_string()
        llm_summary = generate_summary(statistical_summary)
        logging.info("LLM summary generated successfully!")

        # Step 5: Save results to database
        logging.info("Saving results to database...")
        save_summary_to_database(
            start_date_dt.strftime("%Y-%m-%d"),
            llm_summary,
        )

        # Step 6: Prepare JSON-serializable output
        output = {
            "dataset_summary": llm_summary.dataset_summary,  # This is the string needed for comparison_service
            "key_metrics": [
                {"name": metric.name, "value": metric.value}
                for metric in llm_summary.key_metrics
            ],
        }

        logging.info("process_week completed successfully!")

        # Print output for debugging
        print("Output to Q2:", json.dumps(output, indent=4))  # Pretty print JSON output
        return output  # Return JSON-serializable dictionary

    except Exception as e:
        logging.error(f"Error in process_week: {e}")
        raise


def save_summary_to_database(start_date: str, llm_summary: SummaryOutput):
    """
    Saves the structured summary result and its key metrics to the database.

    Args:
        start_date (str): Start date for the summary (YYYY-MM-DD).
        llm_summary (SummaryOutput): The structured summary result.
    """
    try:
        with transaction.atomic():
            logging.info(f"Saving summary for {start_date} to the database...")
            summary = Summary.objects.create(
                start_date=start_date,
                dataset_summary=llm_summary.dataset_summary,
            )
            for metric in llm_summary.key_metrics:
                KeyMetric.objects.create(
                    summary=summary,
                    name=metric.name,
                    value=metric.value,
                )
            logging.info("Summary and key metrics saved successfully.")
    except Exception as e:
        logging.error(f"Failed to save summary and key metrics to the database: {e}")
        raise


# def save_summary_to_file(start_date: str, llm_summary: SummaryOutput):
#     """
#     Saves the structured summary result to a JSON file in the same format as the database.

#     Args:
#         start_date (str): Start date for the summary (YYYY-MM-DD).
#         llm_summary (SummaryOutput): The structured summary result.
#     """
#     try:
#         file_path = f"summary_output_{start_date}.json"
#         logging.info(f"Saving summary result to {file_path}...")
#         data = {
#             "start_date": start_date,
#             "dataset_summary": llm_summary.dataset_summary,
#             "key_metrics": [
#                 {"name": metric.name, "value": metric.value}
#                 for metric in llm_summary.key_metrics
#             ],
#         }
#         with open(file_path, "w") as json_file:
#             json.dump(data, json_file, indent=4)
#         logging.info("Summary result saved successfully.")
#     except Exception as e:
#         logging.error(f"Failed to save summary result to file: {e}")
#         raise

==== data_overview.py ====
# apps/insights/services/csv/data_overview.py


def generate_overview(self, df, label):
    """
    Generate a statistical overview for the given DataFrame.

    Args:
        df (DataFrame): The DataFrame to generate an overview for.
        label (str): A label for logging (e.g., Week 1 or Week 2).
    """
    logging.info(f"Generating statistical overview for {label}...")
    print(f"\nStatistical Overview - {label}:")
    print(df.describe())

==== data_validator.py ====
# apps/insights/services/csv/data_validator.py
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

REQUIRED_COLUMNS = {
    "date",
    "source",
    "sessions",
    "users",
    "new_users",
    "pageviews",
    "pages_per_session",
    "avg_session_duration",
    "bounce_rate",
    "conversion_rate",
    "transactions",
    "revenue",
}


def validate_columns(df):
    # Check for missing required columns
    missing_columns = REQUIRED_COLUMNS - set(df.columns)
    if missing_columns:
        raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")
    logging.info("All required key columns are present.")

==== data_filter.py ====
# apps/insights/services/csv/data_filter.py
import pandas as pd
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def filter_data(df, start_date):
    """
    Filter the DataFrame for a single 7-day period based on the start_date.

    Args:
        df (pd.DataFrame): Input DataFrame to filter.
        start_date (str): Start date for filtering (YYYY-MM-DD).

    Returns:
        pd.DataFrame: Filtered DataFrame for the 7-day period.
    """
    logging.info("Filtering data for organic traffic...")
    organic_df = df[df["source"] == "organic"]
    if organic_df.empty:
        raise ValueError("No data found for organic traffic.")

    # Define date range
    logging.info(f"Calculating date range from start_date: {start_date}")
    start_date = pd.to_datetime(start_date)
    end_date = start_date + pd.Timedelta(days=6)

    # Filter the 7-day period
    logging.info(f"Filtering data: {start_date.date()} to {end_date.date()}")
    filtered_df = organic_df[
        (organic_df["date"] >= start_date) & (organic_df["date"] <= end_date)
    ]
    if filtered_df.empty:
        raise ValueError("No data found for the specified 7-day period.")

    # Log filtered data
    logging.info(f"Filtered Data (Rows: {len(filtered_df)}):\n{filtered_df}")
    return filtered_df

==== csv_reader.py ====
# apps/insights/services/csv/csv_reader.py
import pandas as pd


def load_csv() -> pd.DataFrame:
    """
    Load the CSV file from an absolute path into a Pandas DataFrame.

    Returns:
        pd.DataFrame: Loaded data.
    """
    file_path = "/app/apps/insights/data/ga4_data.csv"  # Absolute path inside container
    try:
        print(f"Attempting to load file from: {file_path}")
        df = pd.read_csv(file_path)
        print(
            f"Successfully loaded {file_path}: {len(df)} rows, {len(df.columns)} columns"
        )
        return df
    except FileNotFoundError:
        raise FileNotFoundError(f"CSV file not found at: {file_path}")
    except Exception as e:
        raise ValueError(f"Error loading CSV file: {e}")

==== __init__.py ====

==== data_cleaner.py ====
# apps/insights/services/csv/data_cleaner.py
import pandas as pd
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


# Detect the date column dynamically
def detect_date_column(df):
    date_columns = [col for col in df.columns if "date" in col.lower()]
    if len(date_columns) == 0:
        raise ValueError("No date column detected in the dataset.")
    if len(date_columns) > 1:
        raise ValueError(f"Multiple possible date columns found: {date_columns}")
    logging.info(f"Date column detected: {date_columns[0]}")
    return date_columns[0]


# Standardize the format of the date column
def standardize_date_format(df, date_column):
    try:
        df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
        if df[date_column].isna().any():
            raise ValueError(f"Invalid or unparseable dates in column '{date_column}'.")
        # Removed the following line to keep 'date' as datetime
        # df[date_column] = df[date_column].dt.strftime("%Y-%m-%d")
        logging.info(
            f"Dates standardized to datetime format in column '{date_column}'."
        )
        return df
    except Exception as e:
        raise ValueError(f"Error standardizing date column: {e}")


# Ensure date column is in datetime format for filtering
def ensure_datetime_format(df, date_column):
    try:
        df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
        if df[date_column].isna().any():
            raise ValueError(f"Invalid or unparseable dates in column '{date_column}'.")
        logging.info(f"Date column '{date_column}' confirmed as datetime format.")
        return df
    except Exception as e:
        raise ValueError(f"Error ensuring datetime format: {e}")


# Perform the full data cleaning process
def clean_data(df):
    date_column = detect_date_column(df)
    df = standardize_date_format(df, date_column)
    df = ensure_datetime_format(df, date_column)
    return df

==== summary_generator.py ====
# apps/insights/services/openai/summary_generator.py

import os
from instructor import from_openai
from openai import OpenAI
from .schemas import SummaryOutput
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load OpenAI API key
openai_api_key = os.environ.get("OPENAI_API_KEY")  # Use os.environ.get()

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment variables.")

# Initialize OpenAI client
client = from_openai(OpenAI(api_key=openai_api_key))


def generate_summary(statistical_summary: str) -> SummaryOutput:
    """
    Generates a structured dataset summary using OpenAI API.

    Args:
        statistical_summary (str): Statistical summary of the dataset.

    Returns:
        SummaryOutput: A structured summary containing dataset insights and key metrics.
    """
    prompt = f"""
You are a data analyst tasked with summarizing a dataset. The following is a statistical summary of the dataset:

{statistical_summary}

Please provide the summary in the following JSON format:

{{
    "dataset_summary": "A concise, insightful summary highlighting significant findings, trends, or patterns observed in the data. Mention any notable data or anomalies in the key metrics, providing context by referencing the actual values and what they indicate about user behavior or performance metrics.",
    "key_metrics": [
        {{
            "name": "Name of Metric",
            "value": Numeric value
        }}
        // Repeat for each key metric
    ]
}}

Ensure that:
- All numeric values are provided as numbers (not strings).
- The key_metrics include the following metrics in this order:
    - "Average Sessions"
    - "Average Users"
    - "Average New Users"
    - "Average Pageviews"
    - "Pages per Session"
    - "Average Session Duration"
    - "Bounce Rate"
    - "Conversion Rate"
    - "Average Transactions"
    - "Average Revenue"
- Do not include descriptions for the key metrics.
- Focus on delivering specific insights derived from the data.
- Avoid generic statements or repeating information without analysis.
"""

    try:
        logging.info("Requesting dataset summary from OpenAI...")

        # API call with structured output validation
        response = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            messages=[{"role": "user", "content": prompt}],
            response_model=SummaryOutput,
        )

        # Log the raw response from OpenAI for debugging
        logging.info(f"Raw LLM response: {response.json()}")

        logging.info("Successfully received structured response.")
        return response

    except client.ValidationError as e:
        logging.error(f"Validation error: {e}")
        raise ValueError(f"Validation error: {e}")

    except client.ApiError as e:
        logging.error(f"API error: {e}")
        raise ValueError(f"API error: {e}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        raise ValueError(f"Unexpected error: {e}")

==== __init__.py ====

==== comparison_generator.py ====
# apps/insights/services/openai/comparison_generator.py

import os
from instructor import from_openai
from openai import OpenAI
from .schemas import ComparisonOutput
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load OpenAI API key
openai_api_key = os.environ.get("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment variables.")

# Initialize OpenAI client
client = from_openai(OpenAI(api_key=openai_api_key))


def generate_comparison(summary1: str, summary2: str) -> ComparisonOutput:
    """
    Generates a structured comparison between two dataset summaries using the OpenAI API.

    Args:
        summary1 (str): The first dataset summary as a string (Week 1).
        summary2 (str): The second dataset summary as a string (Week 2).

    Returns:
        ComparisonOutput: A structured comparison containing a summary and key metrics comparison.
    """
    prompt = f"""
You are a data analyst tasked with comparing two dataset summaries. Here are the summaries:

Week 1:
{summary1}

Week 2:
{summary2}

Please provide the comparison in the following JSON format:

{{
    "comparison_summary": "A concise summary of differences and similarities between Week 1 and Week 2, including notable trends or observations.",
    "key_metrics_comparison": [
        {{
            "name": "Name of Metric",
            "value1": Value from Week 1,
            "value2": Value from Week 2,
            "description": "Description of the observed difference or trend, including specific figures and percentages where appropriate."
        }}
        // Repeat for each key metric
    ]
}}

Ensure that:
- All numeric values are provided as numbers (not strings).
- The key_metrics_comparison includes the following metrics in this order:
    - "Average Sessions"
    - "Average Users"
    - "Average New Users"
    - "Average Pageviews"
    - "Pages per Session"
    - "Average Session Duration"
    - "Bounce Rate"
    - "Conversion Rate"
    - "Average Transactions"
    - "Average Revenue"
- The description for each metric explains the difference or trend observed between Week 1 and Week 2, using precise figures (e.g., differences, percentages).
- Refer to the summaries as "Week 1" and "Week 2" in your descriptions.
"""

    try:
        logging.info("Requesting dataset comparison from OpenAI...")

        # API call with structured output validation
        response = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            messages=[{"role": "user", "content": prompt}],
            response_model=ComparisonOutput,
        )

        # Log the raw response from OpenAI for debugging
        logging.info(f"Raw LLM response: {response.json()}")

        logging.info("Successfully received structured response.")
        return response

    except client.ValidationError as e:
        logging.error(f"Validation error: {e}")
        raise ValueError(f"Validation error: {e}")

    except client.ApiError as e:
        logging.error(f"API error: {e}")
        raise ValueError(f"API error: {e}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        raise ValueError(f"Unexpected error: {e}")

==== schemas.py ====
# apps/insights/services/openai/schemas.py

from pydantic import BaseModel, Field
from typing import List


class KeyMetric(BaseModel):
    """
    Represents a single key metric extracted from the dataset summary.
    """

    name: str
    value: float

    @classmethod
    def ordered_metrics(cls) -> List["KeyMetric"]:
        """
        Defines the exact order and expected names for key metrics.
        """
        return [
            cls(name="Average Sessions", value=0),
            cls(name="Average Users", value=0),
            cls(name="Average New Users", value=0),
            cls(name="Average Pageviews", value=0),
            cls(name="Pages per Session", value=0),
            cls(name="Average Session Duration", value=0),
            cls(name="Bounce Rate", value=0),
            cls(name="Conversion Rate", value=0),
            cls(name="Average Transactions", value=0),
            cls(name="Average Revenue", value=0),
        ]

    def validate_name(self) -> bool:
        """
        Ensures that the name of the metric matches one of the expected names.
        """
        expected_names = [metric.name for metric in self.ordered_metrics()]
        if self.name not in expected_names:
            raise ValueError(f"Unexpected metric name: {self.name}")
        return True


class SummaryOutput(BaseModel):
    """
    Structured output for a dataset summary response from the LLM.
    """

    dataset_summary: str = Field(
        ..., description="A concise English summary of the dataset."
    )
    key_metrics: List[KeyMetric] = Field(
        ..., description="List of key metrics extracted from the dataset."
    )

    def enforce_ordered_metrics(self):
        """
        Enforces that key metrics are in the exact order defined by `KeyMetric.ordered_metrics`.
        """
        ordered_names = [metric.name for metric in KeyMetric.ordered_metrics()]
        self.key_metrics = sorted(
            self.key_metrics,
            key=lambda metric: (
                ordered_names.index(metric.name)
                if metric.name in ordered_names
                else float("inf")
            ),
        )
        # Ensure no unexpected metrics
        for metric in self.key_metrics:
            metric.validate_name()


class KeyMetricComparison(BaseModel):
    """
    Represents a comparison of a key metric between two datasets.
    """

    name: str
    value1: float
    value2: float
    description: str  # Retaining description for comparisons


class ComparisonOutput(BaseModel):
    """
    Structured output for comparing two dataset summaries.
    """

    comparison_summary: str = Field(
        ...,
        description="A concise English summary highlighting differences and similarities between Week 1 and Week 2.",
    )
    key_metrics_comparison: List[KeyMetricComparison] = Field(
        ...,
        description="Key metrics with values from both weeks and descriptions of differences.",
    )
