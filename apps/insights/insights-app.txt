
==== signals.soon.py ====
from django.dispatch import receiver
from django_q.signals import post_execute


@receiver(post_execute)
def create_task_record(sender, task, **kwargs):
    TaskRecord.objects.update_or_create(
        task=task,
        defaults={
            "task_name": task.name or task.func,
            "status": "Success" if task.success else "Failed",
            "started_at": task.started,
            "completed_at": task.stopped,
            "result": str(task.result) if task.result else None,
            "error": str(task.result) if not task.success else None,
        },
    )

==== tasks.py ====
# apps/insights/tasks.py
"""
Task definitions for the Insights app.
These tasks integrate with Django-Q to run asynchronously.
"""

import logging
from datetime import timedelta
from django.utils import timezone
from django_q.tasks import async_task, result_group, fetch
from django_q.models import OrmQ

import pandas as pd

from apps.insights.services.summary_service import process_week
from apps.insights.services.comparison_service import (
    process_comparison,
)  # Corrected import
from apps.insights.services.openai.schemas import SummaryOutput, ComparisonOutput

logger = logging.getLogger(__name__)


def process_week_task(file_path: str, start_date: str, week: int) -> SummaryOutput:
    """
    Processes a single week's data and generates an LLM summary.

    Args:
        file_path (str): Path to the CSV file.
        start_date (str): Start date for the week (YYYY-MM-DD).
        week (int): Week number (1 or 2).

    Returns:
        SummaryOutput: The summary output for the week.
    """
    try:
        logger.info(f"Processing Week {week} starting from {start_date}...")
        result = process_week(file_path, start_date, week)
        logger.info(f"Week {week} summary generated successfully.")
        return result  # Returning the SummaryOutput object directly
    except Exception as e:
        logger.error(f"Failed to process Week {week}: {e}")
        raise


def compare_summaries_task(
    week1_summary: SummaryOutput, week2_summary: SummaryOutput
) -> ComparisonOutput:
    """
    Compares two LLM-generated summaries and generates a comparative analysis.

    Args:
        week1_summary (SummaryOutput): Week 1 summary.
        week2_summary (SummaryOutput): Week 2 summary.

    Returns:
        ComparisonOutput: The comparison output.
    """
    try:
        logger.info("Generating comparison between Week 1 and Week 2 summaries...")
        # Convert SummaryOutput objects to dicts for process_comparison
        summary1_dict = week1_summary.dict()
        summary2_dict = week2_summary.dict()

        comparison_result = process_comparison(summary1_dict, summary2_dict)
        logger.info("Comparison generated successfully.")
        return comparison_result
    except Exception as e:
        logger.error(f"Failed to generate comparison: {e}")
        raise


def schedule_tasks(file_path: str, start_date: str):
    """
    Schedules tasks to process Week 1 and Week 2 sequentially and then compare them.
    """
    try:
        logger.info("Scheduling tasks for Week 1, Week 2, and comparison...")

        # Calculate Week 2 start date
        week2_start_date = pd.to_datetime(start_date) + pd.Timedelta(days=7)
        week2_start_date_str = week2_start_date.strftime("%Y-%m-%d")

        # Schedule Week 1 task with a chain
        async_task(
            "apps.insights.tasks.process_week_task",
            file_path,
            start_date,
            1,
            group="process_summaries",
            hook="apps.insights.tasks.week1_completed_hook",
            kwargs={
                "file_path": file_path,
                "week2_start_date_str": week2_start_date_str,
            },
        )

        logger.info("Tasks scheduled successfully.")

    except Exception as e:
        logger.error(f"Failed to schedule tasks: {e}")
        raise


def week1_completed_hook(task_result, file_path: str, week2_start_date_str: str):
    """
    Hook function called after Week 1 task completes.
    Schedules Week 2 task.

    Args:
        task_result (SummaryOutput): Result from Week 1 processing.
        file_path (str): Path to the CSV file.
        week2_start_date_str (str): Start date for Week 2.
    """
    try:
        logger.info("Week 1 processing completed. Scheduling Week 2 task...")
        # Save Week 1 result to the database or cache if needed
        week1_summary = task_result  # SummaryOutput object

        # Schedule Week 2 task with a hook to run the comparison after completion
        async_task(
            "apps.insights.tasks.process_week_task",
            file_path,
            week2_start_date_str,
            2,
            hook="apps.insights.tasks.week2_completed_hook",
            kwargs={
                "week1_summary": week1_summary,
            },
        )
        logger.info("Week 2 task scheduled successfully.")
    except Exception as e:
        logger.error(f"Failed in week1_completed_hook: {e}")
        raise


def week2_completed_hook(task_result, week1_summary: SummaryOutput):
    """
    Hook function called after Week 2 task completes.
    Runs the comparison task.

    Args:
        task_result (SummaryOutput): Result from Week 2 processing.
        week1_summary (SummaryOutput): Result from Week 1 processing.
    """
    try:
        logger.info("Week 2 processing completed. Running comparison task...")
        week2_summary = task_result  # SummaryOutput object

        # Run comparison task
        comparison_result = compare_summaries_task(week1_summary, week2_summary)

        # Optionally, save comparison_result to the database or handle as needed
        logger.info("Comparison task completed successfully.")
    except Exception as e:
        logger.error(f"Failed in week2_completed_hook: {e}")
        raise

==== data_pipeline.py ====
# apps/insights/data_pipeline.py
"""
Title: Data Pipeline for CSV Processing and LLM Integration
Description:
This script orchestrates the data pipeline for processing GA4 CSV data.
It validates, cleans, filters, and generates statistical overviews,
and optionally integrates with an LLM for dataset summaries.

Usage:
The `run_pipeline` function can be imported and called programmatically:
    from apps.insights.data_pipeline import run_pipeline
"""

import logging
import os
import pandas as pd
from apps.insights.services.csv_processor import CSVProcessor
from apps.insights.services.openai.summary_generator import generate_summary

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def run_pipeline(file_path: str, start_date: str):
    """
    Orchestrates the CSV processing pipeline and outputs results.
    Args:
        file_path (str): Path to the CSV file.
        start_date (str): Starting date for data filtering (YYYY-MM-DD).
    """
    try:
        logging.info("Initializing CSVProcessor...")
        processor = CSVProcessor(file_path)

        logging.info("Starting the CSV processing pipeline...")
        processor.load()
        processor.validate()
        processor.clean()
        week1_df, week2_df = processor.filter(start_date)

        start_date_dt = pd.to_datetime(start_date)
        week1_start = start_date_dt
        week1_end = week1_start + pd.Timedelta(days=6)
        week2_start = week1_end + pd.Timedelta(days=1)
        week2_end = week2_start + pd.Timedelta(days=6)

        logging.info("Generating statistical overviews...")
        logging.info(
            f"\nStatistical Overview - Week 1 (Start: {week1_start.date()}, End: {week1_end.date()}):"
        )
        print(week1_df.describe().to_string())

        logging.info(
            f"\nStatistical Overview - Week 2 (Start: {week2_start.date()}, End: {week2_end.date()}):"
        )
        print(week2_df.describe().to_string())

        logging.info("Generating summaries with OpenAI...")
        week1_summary = week1_df.describe().to_string()
        week2_summary = week2_df.describe().to_string()

        week1_llm_summary = generate_summary(week1_summary)
        week2_llm_summary = generate_summary(week2_summary)

        logging.info(
            f"\nLLM Summary - Week 1 ({week1_start.date()} to {week1_end.date()}):"
        )
        print(week1_llm_summary.dataset_summary)
        logging.info("Key Metrics:")
        for metric in week1_llm_summary.key_metrics:
            print(f"{metric.name}: {metric.value} ({metric.description})")

        logging.info(
            f"\nLLM Summary - Week 2 ({week2_start.date()} to {week2_end.date()}):"
        )
        print(week2_llm_summary.dataset_summary)
        logging.info("Key Metrics:")
        for metric in week2_llm_summary.key_metrics:
            print(f"{metric.name}: {metric.value} ({metric.description})")

        logging.info("Pipeline executed successfully!")

    except Exception as e:
        logging.error(f"Pipeline failed: {e}")
        raise

==== dump.txt ====

==== signals.soon.py ====
from django.dispatch import receiver
from django_q.signals import post_execute


@receiver(post_execute)
def create_task_record(sender, task, **kwargs):
    TaskRecord.objects.update_or_create(
        task=task,
        defaults={
            "task_name": task.name or task.func,
            "status": "Success" if task.success else "Failed",
            "started_at": task.started,
            "completed_at": task.stopped,
            "result": str(task.result) if task.result else None,
            "error": str(task.result) if not task.success else None,
        },
    )

==== tasks.py ====
# apps/insights/tasks.py
"""
Task definitions for the Insights app.
These tasks integrate with Django-Q to run asynchronously.
"""

import logging
from datetime import timedelta
from django.utils import timezone
from django_q.tasks import async_task, result_group, fetch
from django_q.models import OrmQ

import pandas as pd

from apps.insights.services.summary_service import process_week
from apps.insights.services.comparison_service import (
    process_comparison,
)  # Corrected import
from apps.insights.services.openai.schemas import SummaryOutput, ComparisonOutput

logger = logging.getLogger(__name__)


def process_week_task(file_path: str, start_date: str, week: int) -> SummaryOutput:
    """
    Processes a single week's data and generates an LLM summary.

    Args:
        file_path (str): Path to the CSV file.
        start_date (str): Start date for the week (YYYY-MM-DD).
        week (int): Week number (1 or 2).

    Returns:
        SummaryOutput: The summary output for the week.
    """
    try:
        logger.info(f"Processing Week {week} starting from {start_date}...")
        result = process_week(file_path, start_date, week)
        logger.info(f"Week {week} summary generated successfully.")
        return result  # Returning the SummaryOutput object directly
    except Exception as e:
        logger.error(f"Failed to process Week {week}: {e}")
        raise


def compare_summaries_task(
    week1_summary: SummaryOutput, week2_summary: SummaryOutput
) -> ComparisonOutput:
    """
    Compares two LLM-generated summaries and generates a comparative analysis.

    Args:
        week1_summary (SummaryOutput): Week 1 summary.
        week2_summary (SummaryOutput): Week 2 summary.

    Returns:
        ComparisonOutput: The comparison output.
    """
    try:
        logger.info("Generating comparison between Week 1 and Week 2 summaries...")
        # Convert SummaryOutput objects to dicts for process_comparison
        summary1_dict = week1_summary.dict()
        summary2_dict = week2_summary.dict()

        comparison_result = process_comparison(summary1_dict, summary2_dict)
        logger.info("Comparison generated successfully.")
        return comparison_result
    except Exception as e:
        logger.error(f"Failed to generate comparison: {e}")
        raise


def schedule_tasks(file_path: str, start_date: str):
    """
    Schedules tasks to process Week 1 and Week 2 sequentially and then compare them.
    """
    try:
        logger.info("Scheduling tasks for Week 1, Week 2, and comparison...")

        # Calculate Week 2 start date
        week2_start_date = pd.to_datetime(start_date) + pd.Timedelta(days=7)
        week2_start_date_str = week2_start_date.strftime("%Y-%m-%d")

        # Schedule Week 1 task with a chain
        async_task(
            "apps.insights.tasks.process_week_task",
            file_path,
            start_date,
            1,
            group="process_summaries",
            hook="apps.insights.tasks.week1_completed_hook",
            kwargs={
                "file_path": file_path,
                "week2_start_date_str": week2_start_date_str,
            },
        )

        logger.info("Tasks scheduled successfully.")

    except Exception as e:
        logger.error(f"Failed to schedule tasks: {e}")
        raise


def week1_completed_hook(task_result, file_path: str, week2_start_date_str: str):
    """
    Hook function called after Week 1 task completes.
    Schedules Week 2 task.

    Args:
        task_result (SummaryOutput): Result from Week 1 processing.
        file_path (str): Path to the CSV file.
        week2_start_date_str (str): Start date for Week 2.
    """
    try:
        logger.info("Week 1 processing completed. Scheduling Week 2 task...")
        # Save Week 1 result to the database or cache if needed
        week1_summary = task_result  # SummaryOutput object

        # Schedule Week 2 task with a hook to run the comparison after completion
        async_task(
            "apps.insights.tasks.process_week_task",
            file_path,
            week2_start_date_str,
            2,
            hook="apps.insights.tasks.week2_completed_hook",
            kwargs={
                "week1_summary": week1_summary,
            },
        )
        logger.info("Week 2 task scheduled successfully.")
    except Exception as e:
        logger.error(f"Failed in week1_completed_hook: {e}")
        raise


def week2_completed_hook(task_result, week1_summary: SummaryOutput):
    """
    Hook function called after Week 2 task completes.
    Runs the comparison task.

    Args:
        task_result (SummaryOutput): Result from Week 2 processing.
        week1_summary (SummaryOutput): Result from Week 1 processing.
    """
    try:
        logger.info("Week 2 processing completed. Running comparison task...")
        week2_summary = task_result  # SummaryOutput object

        # Run comparison task
        comparison_result = compare_summaries_task(week1_summary, week2_summary)

        # Optionally, save comparison_result to the database or handle as needed
        logger.info("Comparison task completed successfully.")
    except Exception as e:
        logger.error(f"Failed in week2_completed_hook: {e}")
        raise

==== data_pipeline.py ====

==== __init__.py ====

==== apps.py ====
from django.apps import AppConfig


class InsightsConfig(AppConfig):
    default_auto_field = "django.db.models.BigAutoField"
    name = "apps.insights"

==== dump_project.py ====
import os

output_file = "dump.txt"
exclude_dir = "./env"
file_types = (".py", ".js", ".css", ".html", ".yaml", ".json", ".conf", ".txt")

with open(output_file, "w") as out:
    for root, dirs, files in os.walk("."):
        # Exclude the env directory and its subdirectories
        dirs[:] = [d for d in dirs if os.path.join(root, d) != exclude_dir]

        for file in files:
            if file.endswith(file_types):
                file_path = os.path.join(root, file)
                out.write(f"\n==== {file} ====\n")
                with open(file_path, "r", encoding="utf-8") as f:
                    out.write(f.read())

==== admin.py ====
# apps/insights/admin.py
from django.contrib import admin
from .models import DataSummary
from .forms.admin import DataSummaryAdminForm


@admin.register(DataSummary)
class DataSummaryAdmin(admin.ModelAdmin):
    form = DataSummaryAdminForm
    list_display = ("label",)
    search_fields = ("label",)

==== tests.py ====
from django.test import TestCase

# Create your tests here.

==== views.py ====
from django.shortcuts import render

# Create your views here.

==== __init__.py ====

==== 0001_initial.py ====
# Generated by Django 5.1.3 on 2024-11-23 13:12

from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='DataSummary',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('label', models.CharField(help_text="A label identifying the dataset (e.g., 'Week 1').", max_length=50)),
                ('plain_summary', models.TextField(help_text='An English summary of the dataset.')),
                ('key_metrics', models.JSONField(help_text='Structured key metrics from the dataset.')),
                ('metadata', models.JSONField(blank=True, help_text='Additional metadata about the summary (e.g., filters applied).', null=True)),
            ],
            options={
                'verbose_name': 'Data Summary',
                'verbose_name_plural': 'Data Summaries',
            },
        ),
    ]

==== __init__.py ====

==== admin.py ====
# app/insights/forms/admin.py
from django import forms


class DataSummaryAdminForm(forms.ModelForm):
    start_date = forms.DateField(
        widget=forms.widgets.DateInput(attrs={"type": "date"}),
        required=False,
        help_text="Select a start date for the analysis.",
    )

==== comparison_output.json ====
{
    "comparison_summary": "Week 2 showed improvements in several engagement metrics compared to Week 1. Notably, there was an increase in the average sessions, pageviews, transactions, and revenue, while bounce rate and session duration both decreased slightly. The conversion rate saw a significant improvement, indicating a higher effectiveness in user engagement and action completion. However, the number of users and new users remained relatively stable across the two weeks.",
    "key_metrics_comparison": [
        {
            "name": "Average Sessions",
            "value1": 1543.43,
            "value2": 1682.57,
            "description": "The average sessions increased from 1543.43 in Week 1 to 1682.57 in Week 2, indicating a growth of about 9%."
        },
        {
            "name": "Average Users",
            "value1": 1265.14,
            "value2": 1237.86,
            "description": "Average users slightly decreased from 1265.14 in Week 1 to 1237.86 in Week 2, marking a marginal decline, suggesting stable user retention."
        },
        {
            "name": "Average New Users",
            "value1": 427.29,
            "value2": 424.14,
            "description": "The average new users decreased slightly from 427.29 in Week 1 to 424.14 in Week 2, indicating a very small drop in new user acquisition."
        },
        {
            "name": "Average Pageviews",
            "value1": 6225.86,
            "value2": 6891.71,
            "description": "Average pageviews rose from 6225.86 in Week 1 to 6891.71 in Week 2, an increase of approximately 10.7%."
        },
        {
            "name": "Pages per Session",
            "value1": 4.01,
            "value2": 4.07,
            "description": "Pages per session saw a small increase from 4.01 in Week 1 to 4.07 in Week 2, reflecting slightly higher per-session engagement."
        },
        {
            "name": "Average Session Duration",
            "value1": 163.1,
            "value2": 153.88,
            "description": "Average session duration decreased from 163.1 seconds in Week 1 to 153.88 seconds in Week 2, indicating shorter session times."
        },
        {
            "name": "Bounce Rate",
            "value1": 0.2,
            "value2": 0.1606,
            "description": "Bounce rate improved from 20% in Week 1 to 16.06% in Week 2, showing better user retention and engagement."
        },
        {
            "name": "Conversion Rate",
            "value1": 0.028,
            "value2": 0.0425,
            "description": "Conversion rate increased significantly from 2.8% in Week 1 to 4.25% in Week 2, demonstrating enhanced goal achievement."
        },
        {
            "name": "Average Transactions",
            "value1": 34.14,
            "value2": 49.43,
            "description": "Average transactions rose from 34.14 in Week 1 to 49.43 in Week 2, a substantial increase of about 44.8%."
        },
        {
            "name": "Average Revenue",
            "value1": 1622.53,
            "value2": 2087.17,
            "description": "Average revenue increased from $1622.53 in Week 1 to $2087.17 in Week 2, representing a 28.6% boost in earnings."
        }
    ]
}
==== test_summary_service.py ====
# apps/insights/tests/test_summary_service.py

import os
import pytest
from apps.insights.services.summary_service import process_week
from apps.insights.services.openai.schemas import SummaryOutput
from django.conf import settings

print(f"SECRET_KEY in Test: {settings.SECRET_KEY}")


def test_process_week():
    """
    Test the summary service with the actual CSV file and a fixed start date.
    Prints the output for manual verification.
    """
    file_path = os.path.join(os.path.dirname(__file__), "../data/ga4_data.csv")
    start_date = "2024-01-08"
    week_number = 1  # Testing for Week 1

    try:
        # Process the week
        result = process_week(file_path, start_date, week_number)

        # Verify the output type
        assert isinstance(
            result, SummaryOutput
        ), "Result is not a SummaryOutput object."

        # Print dataset summary and key metrics for manual verification
        print("Dataset Summary:")
        print(result.dataset_summary)
        print("\nKey Metrics:")
        for metric in result.key_metrics:
            print(f"{metric.name}: {metric.value}")

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Summary service test failed: {e}")

==== test_comparison_generator.py ====
# apps/insights/tests/test_comparison_generator.py

import pytest
from apps.insights.services.openai.comparison_generator import generate_comparison
from apps.insights.services.openai.schemas import ComparisonOutput, KeyMetricComparison


def test_generate_comparison():
    """
    Test the generate_comparison function using two pre-formatted dataset summary strings.
    Prints the output for manual verification.
    """
    # Pre-formatted dataset summaries
    summary1 = """
    The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
    Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
    such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
    Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
    over the specified time frame.

    Key Metrics:
    - Average Sessions: 1543.43
    - Average Users: 1265.14
    - Average New Users: 427.29
    - Average Pageviews: 6225.86
    - Pages per Session: 4.01
    - Average Session Duration: 163.1
    - Bounce Rate: 0.2
    - Conversion Rate: 0.028
    - Average Transactions: 34.14
    - Average Revenue: 1622.53
    """

    summary2 = """
    The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
    from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
    average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
    approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
    of 6891.71 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
    16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
    in a daily revenue averaging $2087.17.

    Key Metrics:
    - Average Sessions: 1682.57
    - Average Users: 1237.86
    - Average New Users: 424.14
    - Average Pageviews: 6891.71
    - Pages per Session: 4.07
    - Average Session Duration: 153.88
    - Bounce Rate: 0.1606
    - Conversion Rate: 0.0425
    - Average Transactions: 49.43
    - Average Revenue: 2087.17
    """

    try:
        # Call the generator
        result = generate_comparison(summary1, summary2)

        # Verify the output type
        assert isinstance(
            result, ComparisonOutput
        ), "Result is not a ComparisonOutput object."

        # Log and print results for manual verification
        print("Comparison Summary:")
        print(result.comparison_summary)
        print("\nKey Metrics Comparison:")
        for metric in result.key_metrics_comparison:
            print(
                f"{metric.name}: "
                f"Week 1 Value = {metric.value1}, "
                f"Week 2 Value = {metric.value2} "
                f"({metric.description})"
            )

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Comparison generator test failed: {e}")

==== summary_output_week_1.json ====
{
    "dataset_summary": "The dataset captures a week's worth of online activity metrics, indicating overall user engagement and conversion performance. Throughout the period, on average, user sessions and engagement metrics demonstrate consistent user interest, with a mean session count of 1682.57. Although the bounce rate remains relatively low at 16%, suggesting effective content retention, the conversion rate is strikingly low at 4.25%, signaling potential areas for conversion optimization. Furthermore, the variance between new users and returning users highlights a robust core user base, with 1237.86 average total users, including 424.14 new users illustrating ongoing user acquisition efforts. Notably, the revenue generation is lucrative, averaging $2087.17, with an evident peak on the day capturing maximum transactions and revenue influx.",
    "key_metrics": [
        {
            "name": "Average Sessions",
            "value": 1682.57
        },
        {
            "name": "Average Users",
            "value": 1237.86
        },
        {
            "name": "Average New Users",
            "value": 424.14
        },
        {
            "name": "Average Pageviews",
            "value": 6891.71
        },
        {
            "name": "Pages per Session",
            "value": 4.07
        },
        {
            "name": "Average Session Duration",
            "value": 153.88
        },
        {
            "name": "Bounce Rate",
            "value": 0.16
        },
        {
            "name": "Conversion Rate",
            "value": 0.0425
        },
        {
            "name": "Average Transactions",
            "value": 49.43
        },
        {
            "name": "Average Revenue",
            "value": 2087.17
        }
    ]
}
==== comparison_output_old.json ====
{
    "comparison_summary": "Week 1 and Week 2 both cover a 7-day period, capturing web analytic data focused on user engagement and interaction with a website. In Week 2, there is a general increase in sessions, pageviews, transactions, and revenue compared to Week 1, along with a notable decrease in bounce rate and an improved conversion rate. These differences suggest heightened user engagement and effectiveness in converting visits into transactions during Week 2, despite slightly fewer average users and new users compared to Week 1.",
    "key_metrics_comparison": [
        {
            "name": "Average Sessions",
            "value1": 1543.43,
            "value2": 1682.57,
            "description": "There was an increase in average sessions from 1,543 in Week 1 to 1,683 in Week 2, indicating a rise of approximately 9% in user sessions."
        },
        {
            "name": "Average Users",
            "value1": 1265.14,
            "value2": 1237.86,
            "description": "The average users per day decreased slightly from 1,265 in Week 1 to 1,238 in Week 2, showing a minor drop of 2.16%."
        },
        {
            "name": "Average New Users",
            "value1": 427.29,
            "value2": 424.14,
            "description": "Average new users slightly decreased from 427 in Week 1 to 424 in Week 2, a decrease of approximately 0.74%."
        },
        {
            "name": "Average Pageviews",
            "value1": 6225.86,
            "value2": 6891.71,
            "description": "Average pageviews rose significantly from 6,226 in Week 1 to 6,892 in Week 2, reflecting an increase of about 10.7%."
        },
        {
            "name": "Pages per Session",
            "value1": 4.01,
            "value2": 4.07,
            "description": "Pages per session slightly increased from 4.01 in Week 1 to 4.07 in Week 2, indicating a minor improvement in engagement."
        },
        {
            "name": "Average Session Duration",
            "value1": 163.1,
            "value2": 153.88,
            "description": "The average session duration decreased from 163.1 seconds in Week 1 to 153.88 seconds in Week 2, a reduction of 5.64%."
        },
        {
            "name": "Bounce Rate",
            "value1": 0.2,
            "value2": 0.1606,
            "description": "The bounce rate improved, decreasing from 20% in Week 1 to 16% in Week 2, indicating better user retention."
        },
        {
            "name": "Conversion Rate",
            "value1": 0.028,
            "value2": 0.0425,
            "description": "The conversion rate increased significantly from 2.8% in Week 1 to 4.25% in Week 2, showing enhanced capability to convert visitors."
        },
        {
            "name": "Average Transactions",
            "value1": 34.14,
            "value2": 49.43,
            "description": "Average transactions increased from 34 in Week 1 to 49 in Week 2, an increase of 44.7%."
        },
        {
            "name": "Average Revenue",
            "value1": 1622.53,
            "value2": 2087.17,
            "description": "Average revenue saw a substantial rise from $1,623 in Week 1 to $2,087 in Week 2, marking an increase of about 28.64%."
        }
    ]
}
==== test_task_scheduler.py ====
import os
import pytest
import time
from django_q.tasks import async_task, result
import redis


# Ensure migrations are applied for the test database
@pytest.fixture(autouse=True)
def apply_migrations(db):
    from django.core.management import call_command

    call_command("migrate")


@pytest.mark.django_db
def test_process_week_task():
    """
    Test the process_week_task function via async_task and print the result.
    """
    # Ensure critical environment variables are set
    os.environ.setdefault("DJANGO_SECRET_KEY", "test_secret_key")
    os.environ.setdefault("REDIS_URL", "redis://redis:6379/5")
    os.environ.setdefault("REDIS_HOST", "redis")
    os.environ.setdefault("REDIS_PORT", "6379")
    os.environ.setdefault("REDIS_DB", "5")

    # Debug environment variables
    print("DEBUG: Environment Variables:")
    print(f"REDIS_URL: {os.getenv('REDIS_URL')}")
    print(f"REDIS_HOST: {os.getenv('REDIS_HOST')}")
    print(f"REDIS_PORT: {os.getenv('REDIS_PORT')}")
    print(f"REDIS_DB: {os.getenv('REDIS_DB')}")

    # Ensure the Redis client connects properly
    try:
        redis_client = redis.Redis.from_url(os.getenv("REDIS_URL"))
        assert redis_client.ping(), "Redis connection failed!"
        print("DEBUG: Redis connection successful.")
    except Exception as redis_error:
        pytest.fail(f"Redis setup failed: {redis_error}")

    # Correctly resolve the file path for the input data
    file_path = os.path.abspath(
        os.path.join(os.path.dirname(__file__), "../data/ga4_data.csv")
    )
    print(f"DEBUG: Resolved file path for GA4 data: {file_path}")
    start_date = "2024-01-01"  # Example start date
    week_number = 1  # Testing for Week 1

    # Trigger the async task and wait for the result
    try:
        # Trigger the task via Django-Q
        task_id = async_task(
            "apps.insights.tasks.process_week_task",
            file_path,
            start_date,
            week_number,
        )
        print(f"DEBUG: Task {task_id} triggered successfully.")

        # Wait for the result with retries
        retries = 10
        result_data = None
        while retries > 0:
            result_data = result(task_id)
            if result_data is not None:
                break
            retries -= 1
            time.sleep(1)  # Wait 1 second before retrying

        # Validate and print the result
        assert result_data is not None, "Task did not return a result."
        print("DEBUG: Task Result:")
        print(result_data)

    except Exception as task_error:
        pytest.fail(f"Task test failed: {task_error}")

==== test_comparison_service.py ====
# apps/insights/tests/test_comparison_service.py

import pytest
from apps.insights.services.comparison_service import process_comparison
from apps.insights.services.openai.schemas import ComparisonOutput, KeyMetricComparison


def test_process_comparison():
    """
    Test the comparison service using mock structured data for Week 1 and Week 2.
    Prints the output for manual verification.
    """
    # Week 1 structured data
    data_summary1 = {
        "dataset_summary": """
        The dataset covers a 7-day period and encapsulates web analytics data, reflecting user engagement on a website.
        Key metrics include the total number of sessions, users, new users, pageviews, as well as specific engagement metrics 
        such as pages per session, average session duration, bounce rate, conversion rate, transactions, and revenue.
        Overall, the dataset provides an overview of user interaction, revealing patterns in website traffic and user activity
        over the specified time frame.
        """,
        "key_metrics": [
            {
                "name": "Average Sessions",
                "value": 1543.43,
            },
            {
                "name": "Average Users",
                "value": 1265.14,
            },
            {
                "name": "Average New Users",
                "value": 427.29,
            },
            {
                "name": "Average Pageviews",
                "value": 6225.86,
            },
            {
                "name": "Pages per Session",
                "value": 4.01,
            },
            {
                "name": "Average Session Duration",
                "value": 163.1,
            },
            {
                "name": "Bounce Rate",
                "value": 0.2,
            },
            {
                "name": "Conversion Rate",
                "value": 0.028,
            },
            {
                "name": "Average Transactions",
                "value": 34.14,
            },
            {
                "name": "Average Revenue",
                "value": 1622.53,
            },
        ],
    }

    # Week 2 structured data
    data_summary2 = {
        "dataset_summary": """
        The dataset provides a statistical overview of a website's user interaction over a period of seven days in January 2024,
        from the 8th to the 14th. It includes metrics related to sessions, users, new users, pageviews, pages per session, 
        average session duration, bounce rate, conversion rate, transactions, and revenue. The average daily sessions were 
        approximately 1683, with an average of about 1238 users and around 424 new users daily. The website generated an average 
        of 6892 pageviews per day, with each session lasting around 154 seconds on average. The average bounce rate was about 
        16.06%, and the conversion rate stood at about 4.25%. The site recorded an average of 49 transactions per day, resulting
        in a daily revenue averaging $2087.17.
        """,
        "key_metrics": [
            {
                "name": "Average Sessions",
                "value": 1682.57,
            },
            {
                "name": "Average Users",
                "value": 1237.86,
            },
            {
                "name": "Average New Users",
                "value": 424.14,
            },
            {
                "name": "Average Pageviews",
                "value": 6891.71,
            },
            {
                "name": "Pages per Session",
                "value": 4.07,
            },
            {
                "name": "Average Session Duration",
                "value": 153.88,
            },
            {
                "name": "Bounce Rate",
                "value": 0.1606,
            },
            {
                "name": "Conversion Rate",
                "value": 0.0425,
            },
            {
                "name": "Average Transactions",
                "value": 49.43,
            },
            {
                "name": "Average Revenue",
                "value": 2087.17,
            },
        ],
    }

    try:
        # Process comparison
        result = process_comparison(data_summary1, data_summary2)

        # Verify the output type
        assert isinstance(
            result, ComparisonOutput
        ), "Result is not a ComparisonOutput object."

        # Print comparison summary and key metrics for manual verification
        print("Comparison Summary:")
        print(result.comparison_summary)
        print("\nKey Metrics Comparison:")
        for metric in result.key_metrics_comparison:
            print(
                f"{metric.name}: Week 1 Value = {metric.value1}, Week 2 Value = {metric.value2} ({metric.description})"
            )

        print("Test completed successfully.")

    except Exception as e:
        pytest.fail(f"Comparison service test failed: {e}")

==== task.py ====
from django.db import models
from django_q.models import Task


class TaskRecord(models.Model):
    """
    Model to record additional information about tasks executed via Django-Q2.
    """

    task = models.OneToOneField(
        Task,
        on_delete=models.CASCADE,
        help_text="The Django-Q task associated with this record.",
    )
    task_name = models.CharField(
        max_length=255, help_text="Name of the task function being executed."
    )
    status = models.CharField(
        max_length=50, help_text="Status of the task (e.g., Pending, Success, Failed)."
    )
    started_at = models.DateTimeField(help_text="Timestamp when the task started.")
    completed_at = models.DateTimeField(
        null=True, blank=True, help_text="Timestamp when the task completed."
    )
    result = models.TextField(
        null=True, blank=True, help_text="Result of the task, if applicable."
    )
    error = models.TextField(
        null=True, blank=True, help_text="Error message if the task failed."
    )
    summary = models.ForeignKey(
        Summary,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="Associated summary, if applicable.",
    )
    comparison = models.ForeignKey(
        Comparison,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="Associated comparison, if applicable.",
    )
    start_date = models.DateField(
        null=True,
        blank=True,
        help_text="Start date of the data period the task is processing.",
    )
    end_date = models.DateField(
        null=True,
        blank=True,
        help_text="End date of the data period the task is processing.",
    )

    def __str__(self):
        return f"Task Record for Task ID: {self.task.id}"

    class Meta:
        verbose_name = "Task Record"
        verbose_name_plural = "Task Records"
        ordering = ["-started_at"]

==== __init__.py ====
from .summary import DataSummary

__all__ = ["DataSummary"]

==== comparison.py ====
from django.db import models


class Comparison(models.Model):
    """
    Model to store the comparison between two summaries.
    """

    summary1 = models.ForeignKey(
        Summary,
        related_name="comparisons_as_summary1",
        on_delete=models.CASCADE,
        help_text="The first summary being compared.",
    )
    summary2 = models.ForeignKey(
        Summary,
        related_name="comparisons_as_summary2",
        on_delete=models.CASCADE,
        help_text="The second summary being compared.",
    )
    comparison_summary = models.TextField(
        help_text="A concise summary of differences and similarities between the two summaries."
    )
    date_created = models.DateTimeField(
        auto_now_add=True, help_text="Timestamp when the comparison was created."
    )
    date_updated = models.DateTimeField(
        auto_now=True, help_text="Timestamp when the comparison was last updated."
    )

    def __str__(self):
        return f"Comparison from {self.summary1.start_date} to {self.summary2.end_date}"

    class Meta:
        unique_together = ("summary1", "summary2")
        ordering = ["-date_created"]


class KeyMetricComparison(models.Model):
    """
    Model to store individual key metric comparisons related to a Comparison.
    """

    comparison = models.ForeignKey(
        Comparison,
        related_name="key_metrics_comparison",
        on_delete=models.CASCADE,
        help_text="The comparison this key metric comparison belongs to.",
    )
    name = models.CharField(
        max_length=100, help_text="Name of the metric being compared."
    )
    value1 = models.FloatField(help_text="Value from the first summary.")
    value2 = models.FloatField(help_text="Value from the second summary.")
    description = models.TextField(
        help_text="Description of the observed difference or trend."
    )

    def __str__(self):
        return f"{self.name} Comparison (Comparison ID: {self.comparison.id})"

    class Meta:
        unique_together = ("comparison", "name")
        ordering = ["name"]

==== summary.py ====
# apps/insights/models.py

from django.db import models


class Summary(models.Model):
    """
    Model to store the dataset summary and key metrics for a specific time period.
    """

    # Choices for summary type, in case we have summaries for different periods in the future
    SUMMARY_TYPE_CHOICES = [
        ("weekly", "Weekly"),
        ("monthly", "Monthly"),
        # Add more types if needed
    ]

    summary_type = models.CharField(
        max_length=20,
        choices=SUMMARY_TYPE_CHOICES,
        default="weekly",
        help_text="Type of the summary (e.g., weekly, monthly).",
    )
    start_date = models.DateField(help_text="Start date of the data period.")
    end_date = models.DateField(help_text="End date of the data period.")
    dataset_summary = models.TextField(
        help_text="A concise English summary of the dataset."
    )
    # Optionally, store the data source file path or identifier
    data_source = models.CharField(
        max_length=255,
        null=True,
        blank=True,
        help_text="File path or identifier of the data source.",
    )
    date_created = models.DateTimeField(
        auto_now_add=True, help_text="Timestamp when the summary was created."
    )
    date_updated = models.DateTimeField(
        auto_now=True, help_text="Timestamp when the summary was last updated."
    )

    def __str__(self):
        return f"Summary ({self.summary_type.capitalize()}) from {self.start_date} to {self.end_date}"

    class Meta:
        ordering = ["-start_date"]
        unique_together = ("summary_type", "start_date", "end_date")
        verbose_name_plural = "Summaries"


class KeyMetric(models.Model):
    """
    Model to store individual key metrics related to a Summary.
    """

    summary = models.ForeignKey(
        Summary,
        related_name="key_metrics",
        on_delete=models.CASCADE,
        help_text="The summary this key metric belongs to.",
    )
    name = models.CharField(max_length=100, help_text="Name of the metric.")
    value = models.FloatField(help_text="Numeric value of the metric.")

    def __str__(self):
        return f"{self.name}: {self.value} (Summary ID: {self.summary.id})"

    class Meta:
        unique_together = ("summary", "name")
        ordering = ["name"]

==== csv_processor.py ====
# apps/insights/services/csv_processor.py
import logging
from .csv.csv_reader import load_csv
from .csv.data_validator import validate_columns
from .csv.data_cleaner import clean_data
from .csv.data_filter import filter_data
from .csv.data_overview import generate_overview

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class CSVProcessor:
    def __init__(self, file_path: str):
        """
        Initialize the CSVProcessor with the path to the CSV file.
        """
        self.file_path = file_path
        self.df = None  # Placeholder for the loaded DataFrame

    def load(self):
        """
        Load the CSV file into a Pandas DataFrame.
        """
        logging.info("Loading CSV...")
        self.df = load_csv(self.file_path)

    def validate(self):
        """
        Validate that the CSV contains all required columns.
        """
        logging.info("Validating CSV columns...")
        validate_columns(self.df)

    def clean(self):
        """
        Clean the DataFrame by standardizing and formatting columns.
        """
        logging.info("Cleaning data...")
        self.df = clean_data(self.df)

    def filter(self, start_date: str):
        """
        Filter the data into two weeks based on the start date.
        """
        logging.info("Filtering data into Week 1 and Week 2...")
        return filter_data(self.df, start_date)

    def generate_overviews(self, week1_df, week2_df):
        """
        Generate statistical overviews for the filtered DataFrames.
        """
        logging.info("Generating statistical overviews...")
        generate_overview(week1_df, "Week 1")
        generate_overview(week2_df, "Week 2")

    def process(self, start_date: str):
        try:
            self.load()
            self.validate()
            self.clean()
            week1_df, week2_df = self.filter(start_date)
            self.generate_overviews(week1_df, week2_df)
        except ValueError as e:
            logging.error(f"Processing error: {e}")
            raise

==== comparison_service.py ====
# apps/insights/services/comparison_service.py
"""
Comparison Service for Dataset Summaries
Handles LLM comparison generation and logging for two dataset summaries.
"""

import json
import logging
from apps.insights.services.openai.comparison_generator import generate_comparison
from apps.insights.services.openai.schemas import ComparisonOutput

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def prepare_summary(data_summary: dict) -> str:
    """
    Combines dataset_summary and key_metrics from a structured dataset summary into a single string for LLM input.

    Args:
        data_summary (dict): A dictionary containing 'dataset_summary' (str) and 'key_metrics' (list of dicts).

    Returns:
        str: A combined string representation of the dataset summary and its key metrics.
    """
    try:
        if not data_summary.get("dataset_summary"):
            raise ValueError("Missing 'dataset_summary' in data_summary.")

        if not data_summary.get("key_metrics"):
            raise ValueError("Missing 'key_metrics' in data_summary.")

        key_metrics_str = "\n".join(
            f"- {metric['name']}: {metric['value']}"
            for metric in data_summary["key_metrics"]
            if "name" in metric and "value" in metric
        )

        if not key_metrics_str:
            logging.warning("Key metrics are empty or malformed.")

        return f"{data_summary['dataset_summary']}\n\nKey Metrics:\n{key_metrics_str}"
    except Exception as e:
        logging.error(f"Failed to prepare summary: {e}")
        raise


def process_comparison(data_summary1: dict, data_summary2: dict) -> ComparisonOutput:
    """
    Processes two dataset summaries, merges them into strings, and generates a structured comparison.

    Args:
        data_summary1 (dict): The first dataset summary (with 'dataset_summary' and 'key_metrics').
        data_summary2 (dict): The second dataset summary.

    Returns:
        ComparisonOutput: A structured comparison containing a summary and key metrics comparison.
    """
    try:
        logging.info("Starting comparison of dataset summaries...")

        # Validate and prepare text strings for the LLM
        summary1 = prepare_summary(data_summary1)
        summary2 = prepare_summary(data_summary2)

        logging.info("Generated summaries for comparison.")
        logging.debug(f"Summary 1: {summary1}")
        logging.debug(f"Summary 2: {summary2}")

        # Generate comparison using LLM
        comparison_result = generate_comparison(summary1, summary2)

        # Log detailed results
        logging.info("Comparison completed successfully.")
        logging.debug(f"Raw comparison result: {comparison_result}")

        logging.info("Comparison Summary:")
        logging.info(comparison_result.comparison_summary)
        logging.info("Key Metrics Comparison:")
        for metric in comparison_result.key_metrics_comparison:
            logging.info(
                f"{metric.name}: Week 1 Value = {metric.value1}, "
                f"Week 2 Value = {metric.value2} ({metric.description})"
            )

        # Save comparison result to JSON file
        save_comparison_to_file(comparison_result)

        return comparison_result

    except ValueError as ve:
        logging.error(f"Validation Error: {ve}")
        raise

    except Exception as e:
        logging.error(f"Unexpected error during comparison: {e}")
        raise


def save_comparison_to_file(comparison_result: ComparisonOutput):
    """
    Saves the structured comparison result to a JSON file.

    Args:
        comparison_result (ComparisonOutput): The structured comparison result.
    """
    try:
        file_path = "comparison_output.json"
        logging.info(f"Saving comparison result to {file_path}...")
        with open(file_path, "w") as json_file:
            json.dump(comparison_result.dict(), json_file, indent=4)
        logging.info("Comparison result saved successfully.")
    except Exception as e:
        logging.error(f"Failed to save comparison result to file: {e}")
        raise

==== __init__.py ====

==== summary_service.py ====
# apps/insights/services/summary_service.py
"""
Summary Service for Single-Week Data Processing
Handles CSV data validation, processing, LLM summary generation, and key metric extraction for a single week.
"""

import json
import logging
import pandas as pd
from apps.insights.services.csv_processor import CSVProcessor
from apps.insights.services.openai.summary_generator import generate_summary
from apps.insights.services.openai.schemas import SummaryOutput

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def process_week(file_path: str, start_date: str, week_number: int) -> SummaryOutput:
    """
    Processes a single week's data and generates an LLM summary.

    Args:
        file_path (str): Path to the CSV file.
        start_date (str): Start date for the week (YYYY-MM-DD).
        week_number (int): Week number (1 or 2).

    Returns:
        SummaryOutput: LLM summary and key metrics for the week.
    """
    try:
        logging.info(
            f"Processing Week {week_number} data starting from {start_date}..."
        )

        # Initialize CSVProcessor and load data
        processor = CSVProcessor(file_path)
        processor.load()
        processor.validate()
        processor.clean()

        # Filter data for the specified week
        week_df = processor.filter(start_date)[week_number - 1]

        # Log statistical overview
        logging.info(f"Statistical Overview - Week {week_number}:")
        print(week_df.describe().to_string())

        # Generate LLM summary
        logging.info("Requesting summary from OpenAI...")
        statistical_summary = week_df.describe().to_string()
        llm_summary = generate_summary(statistical_summary)

        # Validate and log results
        logging.info(f"LLM Summary - Week {week_number}: {llm_summary.dataset_summary}")
        logging.info("Key Metrics:")
        for metric in llm_summary.key_metrics:
            logging.info(f"{metric.name}: {metric.value}")

        # Save summary to JSON file
        save_summary_to_file(llm_summary, week_number)

        return llm_summary

    except Exception as e:
        logging.error(f"Failed to process Week {week_number}: {e}")
        raise


def save_summary_to_file(summary: SummaryOutput, week_number: int):
    """
    Saves the structured summary result to a JSON file.

    Args:
        summary (SummaryOutput): The structured summary result.
        week_number (int): The week number being processed.
    """
    try:
        file_path = f"summary_output_week_{week_number}.json"
        logging.info(f"Saving Week {week_number} summary result to {file_path}...")
        with open(file_path, "w") as json_file:
            json.dump(summary.dict(), json_file, indent=4)
        logging.info(f"Week {week_number} summary result saved successfully.")
    except Exception as e:
        logging.error(f"Failed to save Week {week_number} summary result to file: {e}")
        raise

==== data_overview.py ====
# apps/insights/services/csv/data_overview.py


def generate_overview(df, label):
    # Generate and print statistical overview for the given DataFrame
    print(f"\nStatistical Overview - {label}:")
    print(df.describe())

==== data_validator.py ====
# apps/insights/services/csv/data_validator.py
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

REQUIRED_COLUMNS = {
    "date",
    "source",
    "sessions",
    "users",
    "new_users",
    "pageviews",
    "pages_per_session",
    "avg_session_duration",
    "bounce_rate",
    "conversion_rate",
    "transactions",
    "revenue",
}


def validate_columns(df):
    # Check for missing required columns
    missing_columns = REQUIRED_COLUMNS - set(df.columns)
    if missing_columns:
        raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")
    logging.info("All required key columns are present.")

==== data_filter.py ====
# apps/insights/services/csv/data_filter.py
import pandas as pd
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def filter_data(df, start_date):
    """
    Filter the DataFrame into two weeks based on the start_date.

    Args:
        df (pd.DataFrame): Input DataFrame to filter.
        start_date (str): Start date for filtering (YYYY-MM-DD).

    Returns:
        tuple: Two DataFrames (Week 1, Week 2).
    """
    logging.info("Filtering data for organic traffic...")
    organic_df = df[df["source"] == "organic"]
    if organic_df.empty:
        raise ValueError("No data found for organic traffic.")

    # Define date ranges
    logging.info(f"Calculating date ranges from start_date: {start_date}")
    start_date = pd.to_datetime(start_date)
    end_date_week1 = start_date + pd.Timedelta(days=6)  # Week 1 range
    start_date_week2 = start_date + pd.Timedelta(days=7)  # Week 2 range
    end_date_week2 = start_date + pd.Timedelta(days=13)  # Week 2 range end

    # Filter Week 1
    logging.info(f"Filtering Week 1: {start_date.date()} to {end_date_week1.date()}")
    week1_df = organic_df[
        (organic_df["date"] >= start_date) & (organic_df["date"] <= end_date_week1)
    ]
    if week1_df.empty:
        raise ValueError("No data found for Week 1.")

    # Log filtered Week 1 data
    logging.info(f"Week 1 Data (Rows: {len(week1_df)}):\n{week1_df}")

    # Filter Week 2
    logging.info(
        f"Filtering Week 2: {start_date_week2.date()} to {end_date_week2.date()}"
    )
    week2_df = organic_df[
        (organic_df["date"] >= start_date_week2)
        & (organic_df["date"] <= end_date_week2)
    ]
    if week2_df.empty:
        raise ValueError("No data found for Week 2.")

    # Log filtered Week 2 data
    logging.info(f"Week 2 Data (Rows: {len(week2_df)}):\n{week2_df}")

    return week1_df, week2_df

==== csv_reader.py ====
# apps/insights/services/csv/csv_reader.py
import pandas as pd  # Missing import added


def load_csv(file_path: str) -> pd.DataFrame:
    """
    Load a CSV file into a Pandas DataFrame.

    Args:
        file_path (str): Path to the CSV file.

    Returns:
        pd.DataFrame: Loaded data.
    """
    try:
        df = pd.read_csv(file_path)
        print(
            f"Successfully loaded {file_path}: {len(df)} rows, {len(df.columns)} columns"
        )
        return df
    except Exception as e:
        raise ValueError(f"Error loading CSV file: {e}")

==== __init__.py ====

==== data_cleaner.py ====
# apps/insights/services/csv/data_cleaner.py
import pandas as pd
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


# Detect the date column dynamically
def detect_date_column(df):
    date_columns = [col for col in df.columns if "date" in col.lower()]
    if len(date_columns) == 0:
        raise ValueError("No date column detected in the dataset.")
    if len(date_columns) > 1:
        raise ValueError(f"Multiple possible date columns found: {date_columns}")
    logging.info(f"Date column detected: {date_columns[0]}")
    return date_columns[0]


# Standardize the format of the date column
def standardize_date_format(df, date_column):
    try:
        df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
        if df[date_column].isna().any():
            raise ValueError(f"Invalid or unparseable dates in column '{date_column}'.")
        df[date_column] = df[date_column].dt.strftime("%Y-%m-%d")
        logging.info(
            f"Dates standardized to 'YYYY-MM-DD' format in column '{date_column}'."
        )
        return df
    except Exception as e:
        raise ValueError(f"Error standardizing date column: {e}")


# Ensure date column is in datetime format for filtering
def ensure_datetime_format(df, date_column):
    try:
        df[date_column] = pd.to_datetime(df[date_column], errors="coerce")
        if df[date_column].isna().any():
            raise ValueError(f"Invalid or unparseable dates in column '{date_column}'.")
        logging.info(f"Date column '{date_column}' converted to datetime format.")
        return df
    except Exception as e:
        raise ValueError(f"Error ensuring datetime format: {e}")


# Perform the full data cleaning process
def clean_data(df):
    date_column = detect_date_column(df)
    df = standardize_date_format(df, date_column)
    df = ensure_datetime_format(df, date_column)
    return df

==== summary.json ====
{
	"dataset_summary": "The dataset provides an overview of website traffic and performance metrics over a seven-day period, from January 8 to January 14, 2024. A noticeable trend in this dataset is a steady increase in sessions, users, and pageviews towards the end of the period, indicative of a growing user engagement. Notably, the bounce rate has a relatively high variation, suggesting inconsistent user interaction with the landing pages. The average session duration also varies significantly, which can influence the overall engagement and conversion rates. The conversion rate, while generally low, shows some improvement, aligning with an increase in transactions and revenue. These metrics suggest potential growth in user acquisition and engagement over the observed period, although further investigation into the high bounce rates could be beneficial to enhance performance further.",
	"key_metrics": [
		{
			"name": "Average Sessions",
			"value": 1682.57,
			"description": "The mean number of sessions per day, indicating the volume of user interactions with the site daily."
		},
		{
			"name": "Average Users",
			"value": 1237.86,
			"description": "The mean number of users per day, showing the average daily reach of the site."
		},
		{
			"name": "Average New Users",
			"value": 424.14,
			"description": "The mean number of new users per day, reflecting the site's ability to attract new visitors."
		},
		{
			"name": "Average Pageviews",
			"value": 6891.71,
			"description": "The mean number of pageviews per day, highlighting the level of user engagement and content consumption."
		},
		{
			"name": "Pages per Session",
			"value": 4.07,
			"description": "The average number of pages viewed during a session, indicating how engaging the content is to users."
		},
		{
			"name": "Average Session Duration",
			"value": 153.88,
			"description": "The average time users spend on the site per session, showing user interest and engagement."
		},
		{
			"name": "Bounce Rate",
			"value": 0.1606,
			"description": "The average percentage of sessions where users left the site without interaction, indicative of landing page effectiveness."
		},
		{
			"name": "Conversion Rate",
			"value": 0.0425,
			"description": "The mean percentage of sessions that resulted in a conversion, reflecting site effectiveness in reaching business goals."
		},
		{
			"name": "Average Transactions",
			"value": 49.43,
			"description": "The average number of transactions per day, showing daily sales activity."
		},
		{
			"name": "Average Revenue",
			"value": 2087.17,
			"description": "The mean daily revenue, demonstrating the site's financial performance over the period."
		}
	]
}

==== summary_generator.py ====
# apps/insights/services/openai/summary_generator.py

import os
from instructor import from_openai
from openai import OpenAI
from .schemas import SummaryOutput
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load OpenAI API key
openai_api_key = os.environ.get("OPENAI_API_KEY")  # Use os.environ.get()

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment variables.")

# Initialize OpenAI client
client = from_openai(OpenAI(api_key=openai_api_key))


def generate_summary(statistical_summary: str) -> SummaryOutput:
    """
    Generates a structured dataset summary using OpenAI API.

    Args:
        statistical_summary (str): Statistical summary of the dataset.

    Returns:
        SummaryOutput: A structured summary containing dataset insights and key metrics.
    """
    prompt = f"""
You are a data analyst tasked with summarizing a dataset. The following is a statistical summary of the dataset:

{statistical_summary}

Please provide the summary in the following JSON format:

{{
    "dataset_summary": "A concise, insightful summary highlighting significant findings, trends, or patterns observed in the data. Mention any notable data or anomalies in the key metrics, providing context by referencing the actual values and what they indicate about user behavior or performance metrics.",
    "key_metrics": [
        {{
            "name": "Name of Metric",
            "value": Numeric value
        }}
        // Repeat for each key metric
    ]
}}

Ensure that:
- All numeric values are provided as numbers (not strings).
- The key_metrics include the following metrics in this order:
    - "Average Sessions"
    - "Average Users"
    - "Average New Users"
    - "Average Pageviews"
    - "Pages per Session"
    - "Average Session Duration"
    - "Bounce Rate"
    - "Conversion Rate"
    - "Average Transactions"
    - "Average Revenue"
- Do not include descriptions for the key metrics.
- Focus on delivering specific insights derived from the data.
- Avoid generic statements or repeating information without analysis.
"""

    try:
        logging.info("Requesting dataset summary from OpenAI...")

        # API call with structured output validation
        response = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            messages=[{"role": "user", "content": prompt}],
            response_model=SummaryOutput,
        )

        # Log the raw response from OpenAI for debugging
        logging.info(f"Raw LLM response: {response.json()}")

        logging.info("Successfully received structured response.")
        return response

    except client.ValidationError as e:
        logging.error(f"Validation error: {e}")
        raise ValueError(f"Validation error: {e}")

    except client.ApiError as e:
        logging.error(f"API error: {e}")
        raise ValueError(f"API error: {e}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        raise ValueError(f"Unexpected error: {e}")

==== __init__.py ====

==== comparison.json ====
{
	"comparison_summary": "Summary 1 and Summary 2 both cover a 7-day period of web analytics, tracking similar key metrics such as sessions, users, pageviews, and revenue. Summary 2 shows a slightly higher average number of sessions and transactions, and notably higher revenue and conversion rates, while Summary 1 has a longer average session duration and higher user numbers.",
	"key_metrics_comparison": [
		{
			"name": "Average Sessions",
			"value1": 1543.43,
			"value2": 1682.57,
			"description": "Summary 2 shows an increase in average sessions compared to Summary 1, indicating improved user engagement."
		},
		{
			"name": "Average Users",
			"value1": 1265.14,
			"value2": 1237.86,
			"description": "Summary 1 reports a slightly higher average number of users than Summary 2."
		},
		{
			"name": "Average New Users",
			"value1": 427.29,
			"value2": 424.14,
			"description": "Summary 1 reports marginally more new users on average compared to Summary 2."
		},
		{
			"name": "Average Pageviews",
			"value1": 6225.86,
			"value2": 6891.71,
			"description": "Summary 2 reports more pageviews on average, suggesting increased interaction with content."
		},
		{
			"name": "Pages per Session",
			"value1": 4.01,
			"value2": 4.07,
			"description": "Both summaries show a similar number of pages per session with a slight increase in Summary 2."
		},
		{
			"name": "Average Session Duration",
			"value1": 163.1,
			"value2": 153.88,
			"description": "Summary 1 has a longer average session duration, indicating users spend more time per visit."
		},
		{
			"name": "Bounce Rate",
			"value1": 0.2,
			"value2": 0.1606,
			"description": "Summary 2 has a lower bounce rate, which suggests more visitors are engaging with multiple pages."
		},
		{
			"name": "Conversion Rate",
			"value1": 0.028,
			"value2": 0.0425,
			"description": "Summary 2 shows a significantly higher conversion rate, implying improved effectiveness in converting visitors."
		},
		{
			"name": "Average Transactions",
			"value1": 34.14,
			"value2": 49.43,
			"description": "Summary 2 depicts a higher number of daily transactions, aligning with its higher conversion rate."
		},
		{
			"name": "Average Revenue",
			"value1": 1622.53,
			"value2": 2087.17,
			"description": "Summary 2 generates more average revenue per day than Summary 1, possibly due to a higher conversion rate and transactions."
		}
	],
	"notable_trends": "The most notable trend is the significant improvement in conversion rate, transactions, and revenue in Summary 2 compared to Summary 1, indicating overall enhanced business performance in the second period."
}

==== comparison_generator.py ====
# apps/insights/services/openai/comparison_generator.py

import os
from instructor import from_openai
from openai import OpenAI
from .schemas import ComparisonOutput
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load OpenAI API key
openai_api_key = os.environ.get("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY is not set in the environment variables.")

# Initialize OpenAI client
client = from_openai(OpenAI(api_key=openai_api_key))


def generate_comparison(summary1: str, summary2: str) -> ComparisonOutput:
    """
    Generates a structured comparison between two dataset summaries using the OpenAI API.

    Args:
        summary1 (str): The first dataset summary as a string (Week 1).
        summary2 (str): The second dataset summary as a string (Week 2).

    Returns:
        ComparisonOutput: A structured comparison containing a summary and key metrics comparison.
    """
    prompt = f"""
You are a data analyst tasked with comparing two dataset summaries. Here are the summaries:

Week 1:
{summary1}

Week 2:
{summary2}

Please provide the comparison in the following JSON format:

{{
    "comparison_summary": "A concise summary of differences and similarities between Week 1 and Week 2, including notable trends or observations.",
    "key_metrics_comparison": [
        {{
            "name": "Name of Metric",
            "value1": Value from Week 1,
            "value2": Value from Week 2,
            "description": "Description of the observed difference or trend, including specific figures and percentages where appropriate."
        }}
        // Repeat for each key metric
    ]
}}

Ensure that:
- All numeric values are provided as numbers (not strings).
- The key_metrics_comparison includes the following metrics in this order:
    - "Average Sessions"
    - "Average Users"
    - "Average New Users"
    - "Average Pageviews"
    - "Pages per Session"
    - "Average Session Duration"
    - "Bounce Rate"
    - "Conversion Rate"
    - "Average Transactions"
    - "Average Revenue"
- The description for each metric explains the difference or trend observed between Week 1 and Week 2, using precise figures (e.g., differences, percentages).
- Refer to the summaries as "Week 1" and "Week 2" in your descriptions.
"""

    try:
        logging.info("Requesting dataset comparison from OpenAI...")

        # API call with structured output validation
        response = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            messages=[{"role": "user", "content": prompt}],
            response_model=ComparisonOutput,
        )

        # Log the raw response from OpenAI for debugging
        logging.info(f"Raw LLM response: {response.json()}")

        logging.info("Successfully received structured response.")
        return response

    except client.ValidationError as e:
        logging.error(f"Validation error: {e}")
        raise ValueError(f"Validation error: {e}")

    except client.ApiError as e:
        logging.error(f"API error: {e}")
        raise ValueError(f"API error: {e}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        raise ValueError(f"Unexpected error: {e}")

==== schemas.py ====
# apps/insights/services/openai/schemas.py

from pydantic import BaseModel, Field
from typing import List


class KeyMetric(BaseModel):
    """
    Represents a single key metric extracted from the dataset summary.
    """

    name: str
    value: float

    @classmethod
    def ordered_metrics(cls) -> List["KeyMetric"]:
        """
        Defines the exact order and expected names for key metrics.
        """
        return [
            cls(name="Average Sessions", value=0),
            cls(name="Average Users", value=0),
            cls(name="Average New Users", value=0),
            cls(name="Average Pageviews", value=0),
            cls(name="Pages per Session", value=0),
            cls(name="Average Session Duration", value=0),
            cls(name="Bounce Rate", value=0),
            cls(name="Conversion Rate", value=0),
            cls(name="Average Transactions", value=0),
            cls(name="Average Revenue", value=0),
        ]

    def validate_name(self) -> bool:
        """
        Ensures that the name of the metric matches one of the expected names.
        """
        expected_names = [metric.name for metric in self.ordered_metrics()]
        if self.name not in expected_names:
            raise ValueError(f"Unexpected metric name: {self.name}")
        return True


class SummaryOutput(BaseModel):
    """
    Structured output for a dataset summary response from the LLM.
    """

    dataset_summary: str = Field(
        ..., description="A concise English summary of the dataset."
    )
    key_metrics: List[KeyMetric] = Field(
        ..., description="List of key metrics extracted from the dataset."
    )

    def enforce_ordered_metrics(self):
        """
        Enforces that key metrics are in the exact order defined by `KeyMetric.ordered_metrics`.
        """
        ordered_names = [metric.name for metric in KeyMetric.ordered_metrics()]
        self.key_metrics = sorted(
            self.key_metrics,
            key=lambda metric: (
                ordered_names.index(metric.name)
                if metric.name in ordered_names
                else float("inf")
            ),
        )
        # Ensure no unexpected metrics
        for metric in self.key_metrics:
            metric.validate_name()


class KeyMetricComparison(BaseModel):
    """
    Represents a comparison of a key metric between two datasets.
    """

    name: str
    value1: float
    value2: float
    description: str  # Retaining description for comparisons


class ComparisonOutput(BaseModel):
    """
    Structured output for comparing two dataset summaries.
    """

    comparison_summary: str = Field(
        ...,
        description="A concise English summary highlighting differences and similarities between Week 1 and Week 2.",
    )
    key_metrics_comparison: List[KeyMetricComparison] = Field(
        ...,
        description="Key metrics with values from both weeks and descriptions of differences.",
    )

==== summary.txt ====
INFO:root:LLM Summary - Week 1: The dataset represents a weekly summary of a website's user engagement and monetization metrics from January 8th to January 14th, 2024. Key performance indicators (KPIs) such as sessions, users, new users, and transactions are presented alongside metrics like bounce rate, conversion rate, and revenue. Over the week, an average of approximately 1,683 sessions were recorded per day with an average session duration of about 154 seconds. The data indicates that there is a steady increase in user engagement, as evidenced by the increment in pageviews which averages at 6,892, and a relatively stable conversion rate of 4.2% leading to a daily average of 49 transactions. Revenue generation shows an average daily intake of roughly $2,087, with fluctuations ranging from $1,449 to $3,030. Notably, the bounce rate is slightly high at 16%, suggesting areas for improvement in retaining user attention. Overall, the metrics demonstrate a healthy digital performance with room for optimization in user engagement and conversion strategies.
INFO:root:Key Metrics:
INFO:root:Sessions: 1682.571429 (Average daily number of sessions indicating user visits.)
INFO:root:Users: 1237.857143 (Average number of unique daily users visiting the website.)
INFO:root:New Users: 424.142857 (Average number of new users per day, reflecting outreach effectiveness.)
INFO:root:Pageviews: 6891.714286 (Average number of pages viewed per day, showing user engagement levels.)
INFO:root:Pages per Session: 4.065714 (Average number of pages viewed per session, indicating content engagement.)
INFO:root:Average Session Duration (seconds): 153.882857 (Average duration of time spent on the website per session.)
INFO:root:Bounce Rate: 0.160614 (Percentage of sessions with only one pageview, implying user engagement challenges.)
INFO:root:Conversion Rate: 0.042486 (Percentage of sessions that resulted in a conversion, indicating success in user actions.)
INFO:root:Transactions: 49.428571 (Average daily number of transactions completed on the website.)
INFO:root:Revenue: 2087.174286 (Average daily revenue generated from user transactions.)
Dataset Summary: